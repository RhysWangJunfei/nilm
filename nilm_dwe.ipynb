{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "nilm_dwe.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RhysWangJunfei/nilm/blob/master/nilm_dwe.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "RrYxX2ExMMhz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import io\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split \n",
        "from google.colab import files"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Xhyx6HgJMWAU",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "'''Sliding window function'''\n",
        "def create_dataset(dataset, look_back=1):\n",
        "    dataX = []\n",
        "    for i in range(len(dataset)-look_back+1):\n",
        "        a = dataset[i:(i+look_back)]\n",
        "        dataX.append(a)\n",
        "    return np.array(dataX)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "SOTCg-ElMW0q",
        "colab_type": "code",
        "outputId": "8f1a97a4-ec13-46b5-c521-a058be0d5e96",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "cell_type": "code",
      "source": [
        "#uploaded = files.upload()\n",
        "'''Load data'''\n",
        "WHE_data = pd.read_csv('Electricity_WHE.csv')['P']\n",
        "#WHE_data = pd.read_csv(io.BytesIO(uploaded['Electricity_WHE.csv']))['P']\n",
        "#uploaded = files.upload()\n",
        "#DWE_data = pd.read_csv(io.BytesIO(uploaded['Electricity_DWE.csv']))['P']\n",
        "DWE_data = pd.read_csv('Electricity_DWE.csv')['P']\n",
        "'''\n",
        "CDE_data = pd.read_csv(base_dir+'Electricity_CDE.csv')['P']\n",
        "CWE_data = pd.read_csv(base_dir+'Electricity_CWE.csv')['P']\n",
        "DWE_data = pd.read_csv(base_dir+'Electricity_DWE.csv')['P']\n",
        "FRE_data = pd.read_csv(base_dir+'Electricity_FRE.csv')['P']\n",
        "HPE_data = pd.read_csv(base_dir+'Electricity_HPE.csv')['P']\n",
        "HTE_data = pd.read_csv(base_dir+'Electricity_HTE.csv')['P']\n",
        "'''"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\nCDE_data = pd.read_csv(base_dir+'Electricity_CDE.csv')['P']\\nCWE_data = pd.read_csv(base_dir+'Electricity_CWE.csv')['P']\\nDWE_data = pd.read_csv(base_dir+'Electricity_DWE.csv')['P']\\nFRE_data = pd.read_csv(base_dir+'Electricity_FRE.csv')['P']\\nHPE_data = pd.read_csv(base_dir+'Electricity_HPE.csv')['P']\\nHTE_data = pd.read_csv(base_dir+'Electricity_HTE.csv')['P']\\n\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "metadata": {
        "id": "zM5ax9XK8hE8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "window_size=60\n",
        "\n",
        "dataX = create_dataset(WHE_data.as_matrix(), window_size)\n",
        "\n",
        "#1-25,110-180,720-840,others\n",
        "\n",
        "dwe_Y = DWE_data[window_size-1:].values.reshape([DWE_data.shape[0]-window_size+1,1])\n",
        "categorized_dwe_Y = np.zeros(dwe_Y.shape)\n",
        "categorized_dwe_Y[[np.where((dwe_Y>0)&(dwe_Y<=25))[0]],:]=1\n",
        "categorized_dwe_Y[[np.where((dwe_Y>110)&(dwe_Y<=180))[0]],:]=2\n",
        "categorized_dwe_Y[[np.where((dwe_Y>720)&(dwe_Y<=840))[0]],:]=3\n",
        "categorized_dwe_Y[[np.where(((dwe_Y>25)&(dwe_Y<110))|((dwe_Y>180)&(dwe_Y<720))|((dwe_Y>840)))[0]],:]=4"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7n-QEPRR8j7R",
        "colab_type": "code",
        "outputId": "e27977a2-a21d-4dd5-eca0-4f1528842767",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 416
        }
      },
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "sns.distplot(categorized_dwe_Y[categorized_dwe_Y>0])\n",
        "plt.title('DWE power consumption distribution')\n",
        "plt.show()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/matplotlib/axes/_axes.py:6521: MatplotlibDeprecationWarning: \n",
            "The 'normed' kwarg was deprecated in Matplotlib 2.1 and will be removed in 3.1. Use 'density' instead.\n",
            "  alternative=\"'density'\", removal=\"3.1\")\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAeMAAAFZCAYAAACrJkcrAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nkf3/8kMqyh59YQIAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 576x396 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "MRNAkQpnMw0D",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "dweY=categorized_dwe_Y[categorized_dwe_Y>0].reshape(-1,1)\n",
        "dweX=dataX[[np.where(categorized_dwe_Y>0)[0]],:][0]\n",
        "\n",
        "encoder = OneHotEncoder(handle_unknown='ignore',sparse=False)\n",
        "dweY_1hot = encoder.fit_transform(dweY)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(dweX, dweY_1hot, test_size=0.1, shuffle=True)\n",
        "scaler = MinMaxScaler()\n",
        "scaler.fit(X_train.astype(float))\n",
        "X_train = scaler.transform(X_train)\n",
        "X_test = scaler.transform(X_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "y72HS0q6MZ0I",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "'''Hyper parameters for deep learning'''\n",
        "# Hyper Parameters\n",
        "LR = 0.001               # learning rate\n",
        "#cfg_list = nf.model_configs()\n",
        "#error_list = []\n",
        "\n",
        "#hyperparameters\n",
        "batch_size=512\n",
        "unit_num=128"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "MxK6ovfgO_NW",
        "colab_type": "code",
        "outputId": "6500e3d2-a55e-4177-c84b-e7c3d31ddac9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 91
        }
      },
      "cell_type": "code",
      "source": [
        "'''RNN Model Definition'''\n",
        "#define inputs\n",
        "tf_x = tf.placeholder(tf.float32, [None, window_size,1])\n",
        "tf_y = tf.placeholder(tf.int32, [None, 4])\n",
        "#Transform from 3d to 2d\n",
        "#X = tf.reshape(tf_x, [-1, 1])\n",
        "#X = tf.reshape(tf_x, [-1, window_size])\n",
        "\n",
        "\n",
        "'''\n",
        "add_layer('l1',X,window_size, 128, activation_function=tf.nn.sigmoid)\n",
        "l2 = add_layer('l2',l1, 128,32 , activation_function=tf.nn.sigmoid)\n",
        "l3 = add_layer('l3',l2, 32,8 , activation_function=tf.nn.sigmoid)\n",
        "pred = add_layer('l4',l3, 8,1 , activation_function=None)\n",
        "'''\n",
        "#X_in = tf.reshape(tf_x, [-1, 1])\n",
        "lstm_cell =tf.contrib.rnn.BasicLSTMCell(num_units=unit_num)\n",
        "outputs, (h_c, h_n) = tf.nn.dynamic_rnn(\n",
        "    lstm_cell,                   # cell you have chosen\n",
        "    tf_x,                      # input\n",
        "    initial_state=None,         # the initial hidden state\n",
        "    dtype=tf.float32,           # must given if set initial_state = None\n",
        "    time_major=False,           # False: (batch, time step, input); True: (time step, batch, input)\n",
        ")\n",
        "l1 = tf.layers.dense(outputs[:, -1, :],64,activation=tf.nn.leaky_relu)\n",
        "l2 = tf.layers.dense(l1,32,activation=tf.nn.leaky_relu)\n",
        "l3 = tf.layers.dense(l2,16,activation=tf.nn.leaky_relu)\n",
        "#l4 = tf.layers.dense(l3,8,activation=tf.nn.leaky_relu)\n",
        "#l5 = tf.layers.dense(l4,4,activation=tf.nn.leaky_relu)\n",
        "#l6 = tf.layers.dense(l5,32,activation=tf.nn.leaky_relu)\n",
        "#l7 = tf.layers.dense(l2,8,activation=tf.nn.leaky_relu)\n",
        "pred = tf.layers.dense(l3,4,activation=tf.nn.relu)\n",
        "#pred = add_layer('dense_1',outputs[:, -1, :], unit_num, 1, activation_function=tf.nn.leaky_relu)\n",
        "'''\n",
        "#dense_2 = add_layer('dense_2',dense_1, 32, 16, activation_function=tf.nn.leaky_relu)\n",
        "#dense_3 = add_layer('dense_3',dense_2, 16, 4, activation_function=tf.nn.leaky_relu)\n",
        "#dense_4 = add_layer('dense_4',dense_1, 1024, 512, activation_function=tf.nn.relu)\n",
        "#dense_5 = add_layer('dense_5',dense_4, 512, 256, activation_function=tf.nn.relu)\n",
        "#dense_6 = add_layer('dense_6',dense_5, 256, 128, activation_function=tf.nn.relu)\n",
        "#dense_7 = add_layer('dense_7',dense_1, 128, 64, activation_function=tf.nn.relu)\n",
        "#dense_8 = add_layer('dense_8',dense_7, 64, 16, activation_function=tf.nn.relu)\n",
        "#dense_9 = add_layer('dense_9',dense_1, 8, 4, activation_function=tf.nn.relu)\n",
        "#pred = add_layer('output', dense_3,4, 1, activation_function=None)\n",
        "'''\n",
        "with tf.name_scope('loss'):\n",
        "    cross_entropy =  tf.nn.softmax_cross_entropy_with_logits_v2(labels=tf_y, logits=pred) \n",
        "    loss = tf.reduce_mean(cross_entropy)\n",
        "    tf.summary.scalar(\"loss\",tensor=loss)\n",
        "\n",
        "train_op = tf.train.AdamOptimizer(LR).minimize(loss)\n",
        "\n",
        "accuracy = tf.metrics.accuracy(          # return (acc, update_op), and create 2 local variables\n",
        "    labels=tf.argmax(tf_y, axis=1), predictions=tf.argmax(pred, axis=1),)[1]\n",
        "\n",
        "'''\n",
        "gvs = optimizer.compute_gradients(loss)\n",
        "capped_gvs = [(tf.clip_by_value(grad, -1., 1.), var) for grad, var in gvs]\n",
        "train_op = optimizer.apply_gradients(capped_gvs) \n",
        "'''\n",
        "init_op = tf.group(tf.global_variables_initializer(), tf.local_variables_initializer()) \n",
        "sess = tf.Session()\n",
        "sess.run(init_op)\n",
        "merged = tf.summary.merge_all()\n",
        "writer = tf.summary.FileWriter(\"logss/\", sess.graph) # tensorflow >=0.12\n",
        "saver = tf.train.Saver()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-11-261980faa897>:17: BasicLSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "This class is deprecated, please use tf.nn.rnn_cell.LSTMCell, which supports all the feature this cell currently has. Please replace the existing code with tf.nn.rnn_cell.LSTMCell(name='basic_lstm_cell').\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "W0Iyar3aPDdy",
        "colab_type": "code",
        "outputId": "80b457a3-3ab7-4b82-a6a3-e8e72a507752",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 6749
        }
      },
      "cell_type": "code",
      "source": [
        "batch_num = len(X_train)//batch_size\n",
        "flag = False\n",
        "for j in range(0,5000):\n",
        "    print('###epoch: '+str(j)+'###')\n",
        "    for i in range(0,batch_num+1):\n",
        "        if(i!=batch_num):\n",
        "            batch_X = X_train[i*batch_size:(i+1)*batch_size,].reshape([batch_size,window_size,1])\n",
        "            batch_Y = y_train[i*batch_size:(i+1)*batch_size,]\n",
        "        else: \n",
        "            batch_X = X_train[-batch_size:].reshape([batch_size,window_size,1])\n",
        "            batch_Y = y_train[-batch_size:,]\n",
        "        sess.run(train_op,{tf_x:batch_X,tf_y:batch_Y})\n",
        "        cost_ = sess.run(loss, {tf_x: batch_X, tf_y:batch_Y})\n",
        "        acc_train = sess.run(accuracy,{tf_x: batch_X, tf_y:batch_Y})\n",
        "        acc_test = sess.run(accuracy,feed_dict={tf_x: X_test.reshape([X_test.shape[0],window_size,1]), tf_y:y_test})\n",
        "        if i%10 == 0:\n",
        "            rs = sess.run(merged,feed_dict={tf_x: batch_X, tf_y:batch_Y})\n",
        "            pre = sess.run(pred,feed_dict={tf_x: batch_X, tf_y:batch_Y})\n",
        "            writer.add_summary(rs, i+j*batch_num)\n",
        "            y_lables_argmax = tf.argmax(tf_y,axis=1)  \n",
        "            y_pred_argmax = tf.argmax(pre,axis=1)\n",
        "            print('###Batch: '+str(i)+': train loss= %.4f' % cost_+', Acc=%.2f'% acc_train)\n",
        "            print('Test Acc=%.2f'% acc_test)\n",
        "            if acc_train>=0.97:\n",
        "              print(j)\n",
        "              save_path = saver.save(sess, \"my_net/save_net_rnn_dwe.ckpt\")\n",
        "              flag = True\n",
        "              break\n",
        "    if(flag==True):\n",
        "        print(flag)\n",
        "        break\n",
        "sess.close()\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "###epoch: 0###\n",
            "###Batch: 0: train loss= 1.3845, Acc=0.12\n",
            "Test Acc=0.10\n",
            "###Batch: 10: train loss= 1.1536, Acc=0.41\n",
            "Test Acc=0.41\n",
            "###Batch: 20: train loss= 1.0773, Acc=0.43\n",
            "Test Acc=0.43\n",
            "###Batch: 30: train loss= 1.0210, Acc=0.43\n",
            "Test Acc=0.43\n",
            "###Batch: 40: train loss= 0.9814, Acc=0.44\n",
            "Test Acc=0.44\n",
            "###Batch: 50: train loss= 1.0427, Acc=0.44\n",
            "Test Acc=0.44\n",
            "###Batch: 60: train loss= 1.0392, Acc=0.44\n",
            "Test Acc=0.44\n",
            "###Batch: 70: train loss= 1.0004, Acc=0.45\n",
            "Test Acc=0.45\n",
            "###epoch: 1###\n",
            "###Batch: 0: train loss= 1.0545, Acc=0.45\n",
            "Test Acc=0.45\n",
            "###Batch: 10: train loss= 1.0729, Acc=0.45\n",
            "Test Acc=0.45\n",
            "###Batch: 20: train loss= 1.0622, Acc=0.45\n",
            "Test Acc=0.45\n",
            "###Batch: 30: train loss= 0.9776, Acc=0.45\n",
            "Test Acc=0.45\n",
            "###Batch: 40: train loss= 0.9474, Acc=0.45\n",
            "Test Acc=0.45\n",
            "###Batch: 50: train loss= 0.9805, Acc=0.45\n",
            "Test Acc=0.45\n",
            "###Batch: 60: train loss= 0.9746, Acc=0.45\n",
            "Test Acc=0.45\n",
            "###Batch: 70: train loss= 0.9552, Acc=0.45\n",
            "Test Acc=0.45\n",
            "###epoch: 2###\n",
            "###Batch: 0: train loss= 0.9789, Acc=0.45\n",
            "Test Acc=0.45\n",
            "###Batch: 10: train loss= 0.9935, Acc=0.45\n",
            "Test Acc=0.45\n",
            "###Batch: 20: train loss= 1.0010, Acc=0.45\n",
            "Test Acc=0.46\n",
            "###Batch: 30: train loss= 0.9607, Acc=0.46\n",
            "Test Acc=0.46\n",
            "###Batch: 40: train loss= 0.9261, Acc=0.47\n",
            "Test Acc=0.47\n",
            "###Batch: 50: train loss= 0.9512, Acc=0.47\n",
            "Test Acc=0.47\n",
            "###Batch: 60: train loss= 0.9492, Acc=0.47\n",
            "Test Acc=0.47\n",
            "###Batch: 70: train loss= 0.9304, Acc=0.48\n",
            "Test Acc=0.48\n",
            "###epoch: 3###\n",
            "###Batch: 0: train loss= 0.9677, Acc=0.48\n",
            "Test Acc=0.48\n",
            "###Batch: 10: train loss= 0.9593, Acc=0.48\n",
            "Test Acc=0.48\n",
            "###Batch: 20: train loss= 0.9472, Acc=0.48\n",
            "Test Acc=0.48\n",
            "###Batch: 30: train loss= 0.9135, Acc=0.49\n",
            "Test Acc=0.49\n",
            "###Batch: 40: train loss= 0.8820, Acc=0.49\n",
            "Test Acc=0.49\n",
            "###Batch: 50: train loss= 0.9237, Acc=0.49\n",
            "Test Acc=0.50\n",
            "###Batch: 60: train loss= 0.9116, Acc=0.50\n",
            "Test Acc=0.50\n",
            "###Batch: 70: train loss= 0.8430, Acc=0.50\n",
            "Test Acc=0.50\n",
            "###epoch: 4###\n",
            "###Batch: 0: train loss= 0.9241, Acc=0.50\n",
            "Test Acc=0.50\n",
            "###Batch: 10: train loss= 0.9072, Acc=0.51\n",
            "Test Acc=0.51\n",
            "###Batch: 20: train loss= 0.9066, Acc=0.51\n",
            "Test Acc=0.51\n",
            "###Batch: 30: train loss= 0.8658, Acc=0.51\n",
            "Test Acc=0.51\n",
            "###Batch: 40: train loss= 0.8359, Acc=0.52\n",
            "Test Acc=0.52\n",
            "###Batch: 50: train loss= 0.8639, Acc=0.52\n",
            "Test Acc=0.52\n",
            "###Batch: 60: train loss= 0.8700, Acc=0.52\n",
            "Test Acc=0.52\n",
            "###Batch: 70: train loss= 0.8275, Acc=0.53\n",
            "Test Acc=0.53\n",
            "###epoch: 5###\n",
            "###Batch: 0: train loss= 0.8472, Acc=0.53\n",
            "Test Acc=0.53\n",
            "###Batch: 10: train loss= 0.8769, Acc=0.53\n",
            "Test Acc=0.53\n",
            "###Batch: 20: train loss= 0.8561, Acc=0.53\n",
            "Test Acc=0.53\n",
            "###Batch: 30: train loss= 0.8252, Acc=0.54\n",
            "Test Acc=0.54\n",
            "###Batch: 40: train loss= 0.7892, Acc=0.54\n",
            "Test Acc=0.54\n",
            "###Batch: 50: train loss= 0.8042, Acc=0.54\n",
            "Test Acc=0.54\n",
            "###Batch: 60: train loss= 0.8281, Acc=0.55\n",
            "Test Acc=0.55\n",
            "###Batch: 70: train loss= 0.7549, Acc=0.55\n",
            "Test Acc=0.55\n",
            "###epoch: 6###\n",
            "###Batch: 0: train loss= 0.8118, Acc=0.55\n",
            "Test Acc=0.55\n",
            "###Batch: 10: train loss= 0.8273, Acc=0.55\n",
            "Test Acc=0.55\n",
            "###Batch: 20: train loss= 0.8259, Acc=0.56\n",
            "Test Acc=0.56\n",
            "###Batch: 30: train loss= 0.7904, Acc=0.56\n",
            "Test Acc=0.56\n",
            "###Batch: 40: train loss= 0.7285, Acc=0.56\n",
            "Test Acc=0.56\n",
            "###Batch: 50: train loss= 0.7449, Acc=0.56\n",
            "Test Acc=0.56\n",
            "###Batch: 60: train loss= 0.7702, Acc=0.57\n",
            "Test Acc=0.57\n",
            "###Batch: 70: train loss= 0.7147, Acc=0.57\n",
            "Test Acc=0.57\n",
            "###epoch: 7###\n",
            "###Batch: 0: train loss= 0.7281, Acc=0.57\n",
            "Test Acc=0.57\n",
            "###Batch: 10: train loss= 0.7851, Acc=0.57\n",
            "Test Acc=0.57\n",
            "###Batch: 20: train loss= 0.7750, Acc=0.58\n",
            "Test Acc=0.58\n",
            "###Batch: 30: train loss= 0.7257, Acc=0.58\n",
            "Test Acc=0.58\n",
            "###Batch: 40: train loss= 0.6766, Acc=0.58\n",
            "Test Acc=0.58\n",
            "###Batch: 50: train loss= 0.6748, Acc=0.58\n",
            "Test Acc=0.58\n",
            "###Batch: 60: train loss= 0.7274, Acc=0.59\n",
            "Test Acc=0.59\n",
            "###Batch: 70: train loss= 0.6462, Acc=0.59\n",
            "Test Acc=0.59\n",
            "###epoch: 8###\n",
            "###Batch: 0: train loss= 0.7008, Acc=0.59\n",
            "Test Acc=0.59\n",
            "###Batch: 10: train loss= 0.6778, Acc=0.59\n",
            "Test Acc=0.59\n",
            "###Batch: 20: train loss= 0.7368, Acc=0.59\n",
            "Test Acc=0.59\n",
            "###Batch: 30: train loss= 0.6598, Acc=0.60\n",
            "Test Acc=0.60\n",
            "###Batch: 40: train loss= 0.6494, Acc=0.60\n",
            "Test Acc=0.60\n",
            "###Batch: 50: train loss= 0.6293, Acc=0.60\n",
            "Test Acc=0.60\n",
            "###Batch: 60: train loss= 0.6525, Acc=0.60\n",
            "Test Acc=0.60\n",
            "###Batch: 70: train loss= 0.5761, Acc=0.60\n",
            "Test Acc=0.60\n",
            "###epoch: 9###\n",
            "###Batch: 0: train loss= 0.6202, Acc=0.61\n",
            "Test Acc=0.61\n",
            "###Batch: 10: train loss= 0.6081, Acc=0.61\n",
            "Test Acc=0.61\n",
            "###Batch: 20: train loss= 0.6239, Acc=0.61\n",
            "Test Acc=0.61\n",
            "###Batch: 30: train loss= 0.5932, Acc=0.61\n",
            "Test Acc=0.61\n",
            "###Batch: 40: train loss= 0.5909, Acc=0.61\n",
            "Test Acc=0.61\n",
            "###Batch: 50: train loss= 0.5240, Acc=0.62\n",
            "Test Acc=0.62\n",
            "###Batch: 60: train loss= 0.6245, Acc=0.62\n",
            "Test Acc=0.62\n",
            "###Batch: 70: train loss= 0.5332, Acc=0.62\n",
            "Test Acc=0.62\n",
            "###epoch: 10###\n",
            "###Batch: 0: train loss= 0.5895, Acc=0.62\n",
            "Test Acc=0.62\n",
            "###Batch: 10: train loss= 0.5563, Acc=0.62\n",
            "Test Acc=0.62\n",
            "###Batch: 20: train loss= 0.6023, Acc=0.63\n",
            "Test Acc=0.63\n",
            "###Batch: 30: train loss= 0.5515, Acc=0.63\n",
            "Test Acc=0.63\n",
            "###Batch: 40: train loss= 0.5730, Acc=0.63\n",
            "Test Acc=0.63\n",
            "###Batch: 50: train loss= 0.4889, Acc=0.63\n",
            "Test Acc=0.63\n",
            "###Batch: 60: train loss= 0.5998, Acc=0.63\n",
            "Test Acc=0.63\n",
            "###Batch: 70: train loss= 0.5154, Acc=0.63\n",
            "Test Acc=0.63\n",
            "###epoch: 11###\n",
            "###Batch: 0: train loss= 0.5750, Acc=0.64\n",
            "Test Acc=0.64\n",
            "###Batch: 10: train loss= 0.5142, Acc=0.64\n",
            "Test Acc=0.64\n",
            "###Batch: 20: train loss= 0.5752, Acc=0.64\n",
            "Test Acc=0.64\n",
            "###Batch: 30: train loss= 0.5240, Acc=0.64\n",
            "Test Acc=0.64\n",
            "###Batch: 40: train loss= 0.5372, Acc=0.64\n",
            "Test Acc=0.64\n",
            "###Batch: 50: train loss= 0.4741, Acc=0.64\n",
            "Test Acc=0.64\n",
            "###Batch: 60: train loss= 0.5893, Acc=0.65\n",
            "Test Acc=0.65\n",
            "###Batch: 70: train loss= 0.5106, Acc=0.65\n",
            "Test Acc=0.65\n",
            "###epoch: 12###\n",
            "###Batch: 0: train loss= 0.5667, Acc=0.65\n",
            "Test Acc=0.65\n",
            "###Batch: 10: train loss= 0.4982, Acc=0.65\n",
            "Test Acc=0.65\n",
            "###Batch: 20: train loss= 0.5610, Acc=0.65\n",
            "Test Acc=0.65\n",
            "###Batch: 30: train loss= 0.5086, Acc=0.65\n",
            "Test Acc=0.65\n",
            "###Batch: 40: train loss= 0.5284, Acc=0.65\n",
            "Test Acc=0.65\n",
            "###Batch: 50: train loss= 0.4629, Acc=0.66\n",
            "Test Acc=0.66\n",
            "###Batch: 60: train loss= 0.5690, Acc=0.66\n",
            "Test Acc=0.66\n",
            "###Batch: 70: train loss= 0.4873, Acc=0.66\n",
            "Test Acc=0.66\n",
            "###epoch: 13###\n",
            "###Batch: 0: train loss= 0.5404, Acc=0.66\n",
            "Test Acc=0.66\n",
            "###Batch: 10: train loss= 0.4790, Acc=0.66\n",
            "Test Acc=0.66\n",
            "###Batch: 20: train loss= 0.5391, Acc=0.66\n",
            "Test Acc=0.66\n",
            "###Batch: 30: train loss= 0.4883, Acc=0.66\n",
            "Test Acc=0.66\n",
            "###Batch: 40: train loss= 0.5079, Acc=0.66\n",
            "Test Acc=0.67\n",
            "###Batch: 50: train loss= 0.4381, Acc=0.67\n",
            "Test Acc=0.67\n",
            "###Batch: 60: train loss= 0.5485, Acc=0.67\n",
            "Test Acc=0.67\n",
            "###Batch: 70: train loss= 0.4644, Acc=0.67\n",
            "Test Acc=0.67\n",
            "###epoch: 14###\n",
            "###Batch: 0: train loss= 0.5102, Acc=0.67\n",
            "Test Acc=0.67\n",
            "###Batch: 10: train loss= 0.4676, Acc=0.67\n",
            "Test Acc=0.67\n",
            "###Batch: 20: train loss= 0.5299, Acc=0.67\n",
            "Test Acc=0.67\n",
            "###Batch: 30: train loss= 0.4840, Acc=0.67\n",
            "Test Acc=0.67\n",
            "###Batch: 40: train loss= 0.4861, Acc=0.67\n",
            "Test Acc=0.67\n",
            "###Batch: 50: train loss= 0.4332, Acc=0.68\n",
            "Test Acc=0.68\n",
            "###Batch: 60: train loss= 0.5594, Acc=0.68\n",
            "Test Acc=0.68\n",
            "###Batch: 70: train loss= 0.4740, Acc=0.68\n",
            "Test Acc=0.68\n",
            "###epoch: 15###\n",
            "###Batch: 0: train loss= 0.5002, Acc=0.68\n",
            "Test Acc=0.68\n",
            "###Batch: 10: train loss= 0.4556, Acc=0.68\n",
            "Test Acc=0.68\n",
            "###Batch: 20: train loss= 0.5087, Acc=0.68\n",
            "Test Acc=0.68\n",
            "###Batch: 30: train loss= 0.4508, Acc=0.68\n",
            "Test Acc=0.68\n",
            "###Batch: 40: train loss= 0.4735, Acc=0.68\n",
            "Test Acc=0.68\n",
            "###Batch: 50: train loss= 0.4140, Acc=0.68\n",
            "Test Acc=0.68\n",
            "###Batch: 60: train loss= 0.5235, Acc=0.69\n",
            "Test Acc=0.69\n",
            "###Batch: 70: train loss= 0.4363, Acc=0.69\n",
            "Test Acc=0.69\n",
            "###epoch: 16###\n",
            "###Batch: 0: train loss= 0.4645, Acc=0.69\n",
            "Test Acc=0.69\n",
            "###Batch: 10: train loss= 0.4302, Acc=0.69\n",
            "Test Acc=0.69\n",
            "###Batch: 20: train loss= 0.4737, Acc=0.69\n",
            "Test Acc=0.69\n",
            "###Batch: 30: train loss= 0.4377, Acc=0.69\n",
            "Test Acc=0.69\n",
            "###Batch: 40: train loss= 0.4512, Acc=0.69\n",
            "Test Acc=0.69\n",
            "###Batch: 50: train loss= 0.3857, Acc=0.69\n",
            "Test Acc=0.69\n",
            "###Batch: 60: train loss= 0.5185, Acc=0.69\n",
            "Test Acc=0.69\n",
            "###Batch: 70: train loss= 0.4121, Acc=0.70\n",
            "Test Acc=0.70\n",
            "###epoch: 17###\n",
            "###Batch: 0: train loss= 0.4583, Acc=0.70\n",
            "Test Acc=0.70\n",
            "###Batch: 10: train loss= 0.4171, Acc=0.70\n",
            "Test Acc=0.70\n",
            "###Batch: 20: train loss= 0.4603, Acc=0.70\n",
            "Test Acc=0.70\n",
            "###Batch: 30: train loss= 0.4157, Acc=0.70\n",
            "Test Acc=0.70\n",
            "###Batch: 40: train loss= 0.4401, Acc=0.70\n",
            "Test Acc=0.70\n",
            "###Batch: 50: train loss= 0.3544, Acc=0.70\n",
            "Test Acc=0.70\n",
            "###Batch: 60: train loss= 0.4867, Acc=0.70\n",
            "Test Acc=0.70\n",
            "###Batch: 70: train loss= 0.3967, Acc=0.70\n",
            "Test Acc=0.70\n",
            "###epoch: 18###\n",
            "###Batch: 0: train loss= 0.4468, Acc=0.70\n",
            "Test Acc=0.70\n",
            "###Batch: 10: train loss= 0.4030, Acc=0.70\n",
            "Test Acc=0.70\n",
            "###Batch: 20: train loss= 0.4375, Acc=0.71\n",
            "Test Acc=0.71\n",
            "###Batch: 30: train loss= 0.3959, Acc=0.71\n",
            "Test Acc=0.71\n",
            "###Batch: 40: train loss= 0.4194, Acc=0.71\n",
            "Test Acc=0.71\n",
            "###Batch: 50: train loss= 0.3437, Acc=0.71\n",
            "Test Acc=0.71\n",
            "###Batch: 60: train loss= 0.4559, Acc=0.71\n",
            "Test Acc=0.71\n",
            "###Batch: 70: train loss= 0.4551, Acc=0.71\n",
            "Test Acc=0.71\n",
            "###epoch: 19###\n",
            "###Batch: 0: train loss= 0.4425, Acc=0.71\n",
            "Test Acc=0.71\n",
            "###Batch: 10: train loss= 0.4286, Acc=0.71\n",
            "Test Acc=0.71\n",
            "###Batch: 20: train loss= 0.4459, Acc=0.71\n",
            "Test Acc=0.71\n",
            "###Batch: 30: train loss= 0.3973, Acc=0.71\n",
            "Test Acc=0.71\n",
            "###Batch: 40: train loss= 0.4131, Acc=0.71\n",
            "Test Acc=0.71\n",
            "###Batch: 50: train loss= 0.3305, Acc=0.72\n",
            "Test Acc=0.72\n",
            "###Batch: 60: train loss= 0.4416, Acc=0.72\n",
            "Test Acc=0.72\n",
            "###Batch: 70: train loss= 0.3658, Acc=0.72\n",
            "Test Acc=0.72\n",
            "###epoch: 20###\n",
            "###Batch: 0: train loss= 0.4332, Acc=0.72\n",
            "Test Acc=0.72\n",
            "###Batch: 10: train loss= 0.3712, Acc=0.72\n",
            "Test Acc=0.72\n",
            "###Batch: 20: train loss= 0.4164, Acc=0.72\n",
            "Test Acc=0.72\n",
            "###Batch: 30: train loss= 0.3830, Acc=0.72\n",
            "Test Acc=0.72\n",
            "###Batch: 40: train loss= 0.3990, Acc=0.72\n",
            "Test Acc=0.72\n",
            "###Batch: 50: train loss= 0.3193, Acc=0.72\n",
            "Test Acc=0.72\n",
            "###Batch: 60: train loss= 0.4299, Acc=0.72\n",
            "Test Acc=0.72\n",
            "###Batch: 70: train loss= 0.3478, Acc=0.72\n",
            "Test Acc=0.72\n",
            "###epoch: 21###\n",
            "###Batch: 0: train loss= 0.4028, Acc=0.72\n",
            "Test Acc=0.72\n",
            "###Batch: 10: train loss= 0.3743, Acc=0.72\n",
            "Test Acc=0.73\n",
            "###Batch: 20: train loss= 0.3918, Acc=0.73\n",
            "Test Acc=0.73\n",
            "###Batch: 30: train loss= 0.3681, Acc=0.73\n",
            "Test Acc=0.73\n",
            "###Batch: 40: train loss= 0.3965, Acc=0.73\n",
            "Test Acc=0.73\n",
            "###Batch: 50: train loss= 0.3069, Acc=0.73\n",
            "Test Acc=0.73\n",
            "###Batch: 60: train loss= 0.4162, Acc=0.73\n",
            "Test Acc=0.73\n",
            "###Batch: 70: train loss= 0.3301, Acc=0.73\n",
            "Test Acc=0.73\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "gQJzAq47tKsG",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}