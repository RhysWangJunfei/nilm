{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ampds.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RhysWangJunfei/nilm/blob/master/ampds.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "F0-Mgc4sN6-o",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split \n",
        "import io\n",
        "from google.colab import files"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xJw01z6kOEsu",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def create_dataset(dataset, look_back=1):\n",
        "    dataX = []\n",
        "    for i in range(len(dataset)-look_back+1):\n",
        "        a = dataset[i:(i+look_back)]\n",
        "        dataX.append(a)\n",
        "    return np.array(dataX)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0VVJngriOJBt",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "uploaded = files.upload()\n",
        "'''Load data'''\n",
        "WHE_data = pd.read_csv(io.BytesIO(uploaded['Electricity_WHE.csv']))['P']\n",
        "uploaded = files.upload()\n",
        "CWE_data = pd.read_csv(io.BytesIO(uploaded['Electricity_CWE.csv']))['P']\n",
        "'''\n",
        "CDE_data = pd.read_csv(base_dir+'Electricity_CDE.csv')['P']\n",
        "CWE_data = pd.read_csv(base_dir+'Electricity_CWE.csv')['P']\n",
        "DWE_data = pd.read_csv(base_dir+'Electricity_DWE.csv')['P']\n",
        "FRE_data = pd.read_csv(base_dir+'Electricity_FRE.csv')['P']\n",
        "HPE_data = pd.read_csv(base_dir+'Electricity_HPE.csv')['P']\n",
        "HTE_data = pd.read_csv(base_dir+'Electricity_HTE.csv')['P']\n",
        "'''"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "WloHw8-OOVFY",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "'''Hyper parameter'''\n",
        "# Hyper Parameters\n",
        "LR = 0.001               # learning rate\n",
        "#cfg_list = nf.model_configs()\n",
        "#error_list = []\n",
        "\n",
        "#hyperparameters\n",
        "window_size=60\n",
        "batch_size=512\n",
        "unit_num=128"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "zUnERexcOaDM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "'''Using sliding window to build training examples'''\n",
        "dataX = create_dataset(WHE_data.as_matrix(), window_size)\n",
        "\n",
        "#get y for the single load\n",
        "#cde_Y = CDE_data[window_size-1:].values.reshape([CDE_data.shape[0]-window_size+1,1])\n",
        "#cdeY=cde_Y[cde_Y>0].reshape(-1,1)\n",
        "#cdeX=dataX[[np.where(cde_Y>0)[0]],:][0]\n",
        "\n",
        "cwe_Y = CWE_data[window_size-1:].values.reshape([CWE_data.shape[0]-window_size+1,1])\n",
        "cweY=cwe_Y[cwe_Y>0].reshape(-1,1)\n",
        "cweX=dataX[[np.where(cwe_Y>0)[0]],:][0]\n",
        "\n",
        "'''\n",
        "cwe_Y = CWE_data[window_size-1:].values.reshape([CDE_data.shape[0]-window_size+1,1])\n",
        "dwe_Y = DWE_data[window_size-1:].values.reshape([CDE_data.shape[0]-window_size+1,1])\n",
        "fre_Y = FRE_data[window_size-1:].values.reshape([CDE_data.shape[0]-window_size+1,1])\n",
        "hpe_Y = HPE_data[window_size-1:].values.reshape([CDE_data.shape[0]-window_size+1,1])\n",
        "hte_Y = HTE_data[window_size-1:].values.reshape([CDE_data.shape[0]-window_size+1,1])\n",
        "dataY = np.concatenate((cde_Y,cwe_Y,dwe_Y,fre_Y,hpe_Y,hte_Y),axis=1)\n",
        "'''"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "SFNoEa0cRvBe",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(cweX, cweY, test_size=0.01, shuffle=True)\n",
        "scaler = MinMaxScaler()\n",
        "scaler.fit(X_train.astype(float))\n",
        "X_train = scaler.transform(X_train)\n",
        "X_test = scaler.transform(X_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "B-zB4ZILRxkB",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "'''RNN Model Definition'''\n",
        "tf.reset_default_graph()\n",
        "#define inputs\n",
        "tf_x = tf.placeholder(tf.float32, [None, window_size,1])\n",
        "tf_y = tf.placeholder(tf.int32, [None, 1])\n",
        "'''\n",
        "def add_layer(name1,inputs,in_size,out_size,activation_function=None):\n",
        "    Weights = tf.get_variable(name1,[in_size, out_size], initializer = tf.ini)\n",
        "    biases = tf.Variable(tf.zeros([1, out_size]) + 0.1)\n",
        "    Wx_plus_b = tf.matmul(inputs, Weights) + biases\n",
        "    if activation_function is None:\n",
        "        outputs = Wx_plus_b\n",
        "    else:\n",
        "        outputs = activation_function(Wx_plus_b)\n",
        "    return outputs\n",
        "'''\n",
        "#Transform from 3d to 2d\n",
        "#X = tf.reshape(tf_x, [-1, 1])\n",
        "#X = tf.reshape(tf_x, [-1, window_size])\n",
        "\n",
        "\n",
        "'''\n",
        "add_layer('l1',X,window_size, 128, activation_function=tf.nn.sigmoid)\n",
        "l2 = add_layer('l2',l1, 128,32 , activation_function=tf.nn.sigmoid)\n",
        "l3 = add_layer('l3',l2, 32,8 , activation_function=tf.nn.sigmoid)\n",
        "pred = add_layer('l4',l3, 8,1 , activation_function=None)\n",
        "'''\n",
        "#X_in = tf.reshape(tf_x, [-1, 1])\n",
        "lstm_cell =tf.contrib.rnn.BasicLSTMCell(num_units=unit_num)\n",
        "outputs, (h_c, h_n) = tf.nn.dynamic_rnn(\n",
        "    lstm_cell,                   # cell you have chosen\n",
        "    tf_x,                      # input\n",
        "    initial_state=None,         # the initial hidden state\n",
        "    dtype=tf.float32,           # must given if set initial_state = None\n",
        "    time_major=False,           # False: (batch, time step, input); True: (time step, batch, input)\n",
        ")\n",
        "l1 = tf.layers.dense(outputs[:, -1, :],64,activation=tf.nn.leaky_relu)\n",
        "l2 = tf.layers.dense(l1,32,activation=tf.nn.leaky_relu)\n",
        "l3 = tf.layers.dense(l2,16,activation=tf.nn.leaky_relu)\n",
        "l4 = tf.layers.dense(l3,8,activation=tf.nn.leaky_relu)\n",
        "l5 = tf.layers.dense(l4,4,activation=tf.nn.leaky_relu)\n",
        "#l6 = tf.layers.dense(l5,32,activation=tf.nn.leaky_relu)\n",
        "#l7 = tf.layers.dense(l2,8,activation=tf.nn.leaky_relu)\n",
        "pred = tf.layers.dense(l5,1,activation=tf.nn.relu)\n",
        "#pred = add_layer('dense_1',outputs[:, -1, :], unit_num, 1, activation_function=tf.nn.leaky_relu)\n",
        "'''\n",
        "#dense_2 = add_layer('dense_2',dense_1, 32, 16, activation_function=tf.nn.leaky_relu)\n",
        "#dense_3 = add_layer('dense_3',dense_2, 16, 4, activation_function=tf.nn.leaky_relu)\n",
        "#dense_4 = add_layer('dense_4',dense_1, 1024, 512, activation_function=tf.nn.relu)\n",
        "#dense_5 = add_layer('dense_5',dense_4, 512, 256, activation_function=tf.nn.relu)\n",
        "#dense_6 = add_layer('dense_6',dense_5, 256, 128, activation_function=tf.nn.relu)\n",
        "#dense_7 = add_layer('dense_7',dense_1, 128, 64, activation_function=tf.nn.relu)\n",
        "#dense_8 = add_layer('dense_8',dense_7, 64, 16, activation_function=tf.nn.relu)\n",
        "#dense_9 = add_layer('dense_9',dense_1, 8, 4, activation_function=tf.nn.relu)\n",
        "#pred = add_layer('output', dense_3,4, 1, activation_function=None)\n",
        "'''\n",
        "with tf.name_scope('loss'):\n",
        "    loss =  tf.reduce_mean(tf.losses.mean_squared_error(tf_y, pred)) \n",
        "    #sigmoid_cross_entropy_with_logits(labels=tf_y, logits=pred)\n",
        "    tf.summary.scalar(\"loss\",tensor=loss)\n",
        "\n",
        "train_op = tf.train.AdamOptimizer(LR).minimize(loss)\n",
        "'''\n",
        "gvs = optimizer.compute_gradients(loss)\n",
        "capped_gvs = [(tf.clip_by_value(grad, -1., 1.), var) for grad, var in gvs]\n",
        "train_op = optimizer.apply_gradients(capped_gvs) \n",
        "'''\n",
        "init_op = tf.group(tf.global_variables_initializer(), tf.local_variables_initializer()) \n",
        "sess = tf.Session()\n",
        "sess.run(init_op)\n",
        "merged = tf.summary.merge_all()\n",
        "writer = tf.summary.FileWriter(\"logss/\", sess.graph) # tensorflow >=0.12"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ql8SXcanU2V6",
        "colab_type": "code",
        "outputId": "256de8a0-7fbc-47b1-a29c-49105c5625a3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "batch_num = len(X_train)//batch_size\n",
        "for j in range(0,6000):\n",
        "    for i in range(0,batch_num+1):\n",
        "        if(i!=batch_num):\n",
        "            batch_X = X_train[i*batch_size:(i+1)*batch_size,].reshape([batch_size,window_size,1])\n",
        "            batch_Y = y_train[i*batch_size:(i+1)*batch_size,]\n",
        "        else: \n",
        "            batch_X = X_train[-batch_size:].reshape([batch_size,window_size,1])\n",
        "            batch_Y = y_train[-batch_size:,]\n",
        "        #tmp = cf.get_change_points_binary_vector(batch_X)\n",
        "        sess.run(train_op,{tf_x:batch_X,tf_y:batch_Y})\n",
        "        cost_ = sess.run(loss, {tf_x: batch_X, tf_y:batch_Y})\n",
        "        if i%10 == 0:\n",
        "            rs = sess.run(merged,feed_dict={tf_x: batch_X, tf_y:batch_Y})\n",
        "            pre = sess.run(pred,feed_dict={tf_x: batch_X, tf_y:batch_Y})\n",
        "            writer.add_summary(rs, i+j*batch_num)\n",
        "            print('###############epoch: '+str(j)+' iter: '+str(i)+' ,train loss: %.4f' % cost_+'#############')\n",
        "            #vars = tf.trainable_variables()\n",
        "            #print(vars) #some infos about variables...\n",
        "            #vars_vals = sess.run(vars)\n",
        "            #for var, val in zip(vars, vars_vals):\n",
        "            #    print(\"var: {}, value: {}\".format(var.name, val))\n",
        "            #print(pred.eval(session=sess, feed_dict={tf_x: X_test.reshape([X_test.shape[0],window_size,1]),tf_y: y_test}))\n",
        "loss = sess.run(loss, feed_dict={tf_x: X_test.reshape([X_test.shape[0],window_size,1]), tf_y:y_test})\n",
        "pre = sess.run(pred,feed_dict={tf_x: X_test.reshape([X_test.shape[0],window_size,1]), tf_y: y_test})\n",
        "print('test accuracy: %.2f'% loss)\n",
        "print(y_test)\n",
        "print(pre)\n",
        "sess.close()\n",
        "#tensorboard --logdir==mylogs:D:\\tech\\Workspaces\\eclipse-workspace\\NILM\\logss\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " > Downloading \"/Electricity_CWE.csv\" to \"/content/Dropbox-Uploader/Electricity_CWE.csv\"... DONE\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}