{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "nilm_rnn_cde.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RhysWangJunfei/nilm/blob/master/nilm_rnn_cde.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "10LlCP6Ka3U_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import io\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split \n",
        "from google.colab import files\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "uwWj9j_5bEyn",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "'''Sliding window function'''\n",
        "def create_dataset(dataset, look_back=1):\n",
        "    dataX = []\n",
        "    for i in range(len(dataset)-look_back+1):\n",
        "        a = dataset[i:(i+look_back)]\n",
        "        dataX.append(a)\n",
        "    return np.array(dataX)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "VFVwodqibHz0",
        "colab_type": "code",
        "outputId": "b44c4852-f5a4-48e1-d2d0-a58ac569eaeb",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": "OK"
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 78
        }
      },
      "cell_type": "code",
      "source": [
        "uploaded = files.upload()\n",
        "'''Load data'''\n",
        "WHE_data = pd.read_csv(io.BytesIO(uploaded['Electricity_WHE.csv']))['P']\n",
        "#WHE_data = pd.read_csv('Electricity_WHE.csv')['P']"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-600b0663-4c28-4f4b-85d7-33f9bd3a70d7\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-600b0663-4c28-4f4b-85d7-33f9bd3a70d7\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving Electricity_WHE.csv to Electricity_WHE.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "GD3aL2xWT7NC",
        "colab_type": "code",
        "outputId": "a5db687e-f720-4e2d-b93d-f52054866409",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": "OK"
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 78
        }
      },
      "cell_type": "code",
      "source": [
        "uploaded = files.upload()\n",
        "CDE_data = pd.read_csv(io.BytesIO(uploaded['Electricity_CDE.csv']))['P']\n",
        "#CDE_data = pd.read_csv('Electricity_CDE.csv')['P']"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-1a14b220-6afd-4769-ac26-a3de5a0ce1cd\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-1a14b220-6afd-4769-ac26-a3de5a0ce1cd\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving Electricity_CDE.csv to Electricity_CDE.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "tav03ikTb6wo",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "window_size=60\n",
        "\n",
        "dataX_raw = create_dataset(WHE_data.as_matrix(), window_size)\n",
        "\n",
        "#0,1-350,4401-5300,others\n",
        "\n",
        "cde_Y_raw = CDE_data[window_size-1:].values.reshape([CDE_data.shape[0]-window_size+1,1])\n",
        "dataX = np.concatenate([dataX_raw[0:472500,:],dataX_raw[475500:,:]],axis=0)\n",
        "cde_Y = np.concatenate([cde_Y_raw[0:472500,:],cde_Y_raw[475500:,:]],axis=0)\n",
        "categorized_cde_Y = np.ones(cde_Y.shape)*3\n",
        "categorized_cde_Y[[np.where(cde_Y==0)[0]],:]=0\n",
        "categorized_cde_Y[[np.where((cde_Y>0)&(cde_Y<=350))[0]],:]=1\n",
        "categorized_cde_Y[[np.where((cde_Y>4400)&(cde_Y<=5300))[0]],:]=2\n",
        "\n",
        "#cdeY=categorized_cde_Y[categorized_cde_Y>0].reshape(-1,1)\n",
        "#cdeX=dataX[[np.where(categorized_cde_Y>0)[0]],:][0]\n",
        "\n",
        "\n",
        "encoder = OneHotEncoder(handle_unknown='ignore',sparse=False)\n",
        "cdeY_1hot = encoder.fit_transform(categorized_cde_Y)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(dataX, cdeY_1hot, test_size=0.1, shuffle=True)\n",
        "scaler = MinMaxScaler()\n",
        "scaler.fit(X_train.astype(float))\n",
        "X_train = scaler.transform(X_train)\n",
        "X_test = scaler.transform(X_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "cwiBQAW7b8vj",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "tl = RandomUnderSampler(sampling_strategy={0:9000,1:9000,2:9000,3:0})\n",
        "X_train, y_train = tl.fit_resample(X_train, y_train)\n",
        "whole_train = np.concatenate([X_train,y_train],axis=1)\n",
        "#from imblearn.under_sampling import EditedNearestNeighbours\n",
        "#enn = EditedNearestNeighbours()\n",
        "#X_train, y_train = enn.fit_resample(X_train, y_train)\n",
        "\n",
        "#from imblearn.under_sampling import NeighbourhoodCleaningRule\n",
        "#ncr = NeighbourhoodCleaningRule()\n",
        "#X_train, y_train = ncr.fit_resample(X_train, y_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "IuitJP4wl2q1",
        "colab_type": "code",
        "outputId": "09d94c86-dbec-4be7-9276-eb4134d74223",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 109
        }
      },
      "cell_type": "code",
      "source": [
        "max_indice = np.argmax(cdeY_1hot,1)\n",
        "df = pd.Series(max_indice)\n",
        "df.value_counts()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    1026791\n",
              "2      11216\n",
              "1      10095\n",
              "3         39\n",
              "dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "metadata": {
        "id": "nCs4xO2_o-P1",
        "colab_type": "code",
        "outputId": "d95e31a2-fa28-4f96-bb07-8bb0ac828e82",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "y_train.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(27000, 4)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "metadata": {
        "id": "xmXT4R3ZcD5d",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "'''Hyper parameters for deep learning'''\n",
        "# Hyper Parameters\n",
        "LR = 0.001               # learning rate\n",
        "#cfg_list = nf.model_configs()\n",
        "#error_list = []\n",
        "\n",
        "#hyperparameters\n",
        "batch_size=512\n",
        "unit_num=128"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "rtarjIN1cE1p",
        "colab_type": "code",
        "outputId": "5d54f1ef-3ba4-4e76-c36c-9aa7d0040df4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 386
        }
      },
      "cell_type": "code",
      "source": [
        "'''RNN Model Definition'''\n",
        "tf.reset_default_graph()\n",
        "''''''\n",
        "#define inputs\n",
        "tf_x = tf.placeholder(tf.float32, [None, window_size,1],name='x')\n",
        "tf_y = tf.placeholder(tf.int32, [None, 4],name='y')\n",
        "\n",
        "lstm_cell =tf.contrib.rnn.BasicLSTMCell(num_units=unit_num,name='lstm_cell')\n",
        "outputs, (h_c, h_n) = tf.nn.dynamic_rnn(\n",
        "    lstm_cell,                   # cell you have chosen\n",
        "    tf_x,                      # input\n",
        "    initial_state=None,         # the initial hidden state\n",
        "    dtype=tf.float32,           # must given if set initial_state = None\n",
        "    time_major=False,           # False: (batch, time step, input); True: (time step, batch, input)\n",
        ")\n",
        "l1 = tf.layers.dense(outputs[:, -1, :],512,activation=tf.nn.leaky_relu,name='l1')\n",
        "l2 = tf.layers.dense(l1,1024,activation=tf.nn.leaky_relu,name='l2')\n",
        "l3 = tf.layers.dense(l2,512,activation=tf.nn.leaky_relu,name='l3')\n",
        "l4 = tf.layers.dense(l3,256,activation=tf.nn.leaky_relu,name='l4')\n",
        "l5 = tf.layers.dense(l4,128,activation=tf.nn.leaky_relu,name='l5')\n",
        "l6 = tf.layers.dense(l5,84,activation=tf.nn.leaky_relu,name='l6')\n",
        "l7 = tf.layers.dense(l6,64,activation=tf.nn.leaky_relu,name='l7')\n",
        "l8 = tf.layers.dense(l7,48,activation=tf.nn.leaky_relu,name='l8')\n",
        "l9 = tf.layers.dense(l8,32,activation=tf.nn.leaky_relu,name='l9')\n",
        "l10 = tf.layers.dense(l9,24,activation=tf.nn.leaky_relu,name='l10')\n",
        "l11 = tf.layers.dense(l10,16,activation=tf.nn.leaky_relu,name='l11')\n",
        "l12 = tf.layers.dense(l11,8,activation=tf.nn.leaky_relu,name='l12')\n",
        "pred = tf.layers.dense(l12,4,activation=tf.nn.relu,name='pred')\n",
        "\n",
        "with tf.name_scope('loss'):\n",
        "    cross_entropy =  tf.nn.softmax_cross_entropy_with_logits_v2(labels=tf_y, logits=pred) \n",
        "    loss = tf.reduce_mean(cross_entropy)\n",
        "    tf.summary.scalar(\"loss\",tensor=loss)\n",
        "\n",
        "train_op = tf.train.AdamOptimizer(LR).minimize(loss)\n",
        "\n",
        "accuracy = tf.reduce_mean(tf.cast(tf.equal(tf.argmax(tf_y, axis=1), tf.argmax(pred, axis=1)), tf.float32))\n",
        "\n",
        "init_op = tf.group(tf.global_variables_initializer(), tf.local_variables_initializer()) \n",
        "saver = tf.train.Saver()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n",
            "WARNING:tensorflow:From <ipython-input-12-50c3d6b8abd0>:8: BasicLSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "This class is equivalent as tf.keras.layers.LSTMCell, and will be replaced by that in Tensorflow 2.0.\n",
            "WARNING:tensorflow:From <ipython-input-12-50c3d6b8abd0>:14: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/tensor_array_ops.py:162: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "WARNING:tensorflow:From <ipython-input-12-50c3d6b8abd0>:16: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.dense instead.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "DU9K7Jw1cJZw",
        "colab_type": "code",
        "outputId": "df1696f3-26f2-4781-894e-21a6e868bb74",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 21782
        }
      },
      "cell_type": "code",
      "source": [
        "sess = tf.Session()\n",
        "#sess.run(init_op)\n",
        "saver.restore(sess, 'my_net/save_net_rnn.ckpt')\n",
        "\n",
        "flag = False\n",
        "for j in range(0,300):\n",
        "    print('###iteration: '+str(j)+'###')\n",
        "    batch_index = np.random.choice(27000,batch_size)\n",
        "    batch_train = whole_train[batch_index,:]\n",
        "    batch_X = batch_train[:,:-4].reshape([batch_size,window_size,1])\n",
        "    batch_y = batch_train[:,-4:]\n",
        "    sess.run(train_op,{tf_x:batch_X , tf_y:batch_y})\n",
        "    cost_ = sess.run(loss,{tf_x:batch_X, tf_y:batch_y})\n",
        "    acc_train = sess.run(accuracy,{tf_x:batch_X, tf_y:batch_y})\n",
        "    acc_test = sess.run(accuracy,feed_dict={tf_x: X_test.reshape([X_test.shape[0],window_size,1]), tf_y:y_test})\n",
        "    print('train loss= %.4f' % cost_+', Acc=%.2f'% acc_train)\n",
        "    print('Test Acc=%.2f'% acc_test)\n",
        "    \n",
        "    pre = sess.run(pred,feed_dict={tf_x: batch_X, tf_y:batch_y})\n",
        "    y_lables_argmax = tf.argmax(tf_y,axis=1)  \n",
        "    y_pred_argmax = tf.argmax(pre,axis=1)\n",
        "    confusion = tf.confusion_matrix(labels=y_lables_argmax, predictions=y_pred_argmax, num_classes=4)\n",
        "    #print('Confusion Matrix: \\n\\n', tf.Tensor.eval(confusion,feed_dict=None))\n",
        "    print(confusion.eval(session=sess,feed_dict={tf_x: batch_X, tf_y:batch_y}))\n",
        "    if acc_train>=0.99:\n",
        "      print(j)\n",
        "      flag = True\n",
        "      break\n",
        "    if(flag==True):\n",
        "        print(flag)\n",
        "        break\n",
        "    if(j//100==0):\n",
        "      save_path = saver.save(sess, \"my_net/save_net_rnn.ckpt\")\n",
        "pre = sess.run(pred,feed_dict={tf_x: X_test.reshape([X_test.shape[0],window_size,1]), tf_y: y_test})\n",
        "y_lables_argmax = np.argmax(y_test,1)\n",
        "y_pred_argmax = np.argmax(pre,1)\n",
        "confusion = tf.confusion_matrix(labels=y_lables_argmax, predictions=y_pred_argmax, num_classes=4)\n",
        "#print('Confusion Matrix: \\n\\n', tf.Tensor.eval(confusion,feed_dict=None))\n",
        "print(confusion.eval(session=sess))\n",
        "sess.close()\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Restoring parameters from my_net/save_net_rnn.ckpt\n",
            "###iteration: 0###\n",
            "train loss= 0.2972, Acc=0.87\n",
            "Test Acc=0.92\n",
            "[[141  11   1   0]\n",
            " [ 51 135   3   0]\n",
            " [  0   0 170   0]\n",
            " [  0   0   0   0]]\n",
            "###iteration: 1###\n",
            "train loss= 0.2911, Acc=0.88\n",
            "Test Acc=0.92\n",
            "[[151  13   1   0]\n",
            " [ 46 122   4   0]\n",
            " [  0   0 175   0]\n",
            " [  0   0   0   0]]\n",
            "###iteration: 2###\n",
            "train loss= 0.2668, Acc=0.89\n",
            "Test Acc=0.94\n",
            "[[146   9   1   0]\n",
            " [ 43 137   0   0]\n",
            " [  0   1 175   0]\n",
            " [  0   0   0   0]]\n",
            "###iteration: 3###\n",
            "train loss= 0.2909, Acc=0.89\n",
            "Test Acc=0.95\n",
            "[[164   5   1   0]\n",
            " [ 50 120   1   0]\n",
            " [  0   0 171   0]\n",
            " [  0   0   0   0]]\n",
            "###iteration: 4###\n",
            "train loss= 0.2848, Acc=0.88\n",
            "Test Acc=0.94\n",
            "[[164   7   1   0]\n",
            " [ 52 112   2   0]\n",
            " [  0   1 173   0]\n",
            " [  0   0   0   0]]\n",
            "###iteration: 5###\n",
            "train loss= 0.2977, Acc=0.87\n",
            "Test Acc=0.94\n",
            "[[145   9   0   0]\n",
            " [ 55 140   0   0]\n",
            " [  0   1 162   0]\n",
            " [  0   0   0   0]]\n",
            "###iteration: 6###\n",
            "train loss= 0.2864, Acc=0.89\n",
            "Test Acc=0.93\n",
            "[[178   8   0   0]\n",
            " [ 46 111   2   0]\n",
            " [  0   0 167   0]\n",
            " [  0   0   0   0]]\n",
            "###iteration: 7###\n",
            "train loss= 0.2973, Acc=0.88\n",
            "Test Acc=0.92\n",
            "[[150  11   0   0]\n",
            " [ 49 124   2   0]\n",
            " [  0   0 176   0]\n",
            " [  0   0   0   0]]\n",
            "###iteration: 8###\n",
            "train loss= 0.2703, Acc=0.90\n",
            "Test Acc=0.93\n",
            "[[155  10   1   0]\n",
            " [ 39 133   1   0]\n",
            " [  0   0 173   0]\n",
            " [  0   0   0   0]]\n",
            "###iteration: 9###\n",
            "train loss= 0.2623, Acc=0.90\n",
            "Test Acc=0.92\n",
            "[[163   6   1   0]\n",
            " [ 44 121   1   0]\n",
            " [  0   0 176   0]\n",
            " [  0   0   0   0]]\n",
            "###iteration: 10###\n",
            "train loss= 0.2751, Acc=0.89\n",
            "Test Acc=0.93\n",
            "[[171  11   0   0]\n",
            " [ 45 119   2   0]\n",
            " [  0   0 164   0]\n",
            " [  0   0   0   0]]\n",
            "###iteration: 11###\n",
            "train loss= 0.3048, Acc=0.87\n",
            "Test Acc=0.94\n",
            "[[171   6   0   0]\n",
            " [ 59 114   2   0]\n",
            " [  0   0 160   0]\n",
            " [  0   0   0   0]]\n",
            "###iteration: 12###\n",
            "train loss= 0.2812, Acc=0.88\n",
            "Test Acc=0.95\n",
            "[[196   8   0   0]\n",
            " [ 51 115   0   0]\n",
            " [  0   0 142   0]\n",
            " [  0   0   0   0]]\n",
            "###iteration: 13###\n",
            "train loss= 0.2772, Acc=0.89\n",
            "Test Acc=0.95\n",
            "[[164   5   1   0]\n",
            " [ 50 125   1   0]\n",
            " [  0   0 166   0]\n",
            " [  0   0   0   0]]\n",
            "###iteration: 14###\n",
            "train loss= 0.3125, Acc=0.89\n",
            "Test Acc=0.94\n",
            "[[174   8   3   0]\n",
            " [ 43 122   2   0]\n",
            " [  0   0 160   0]\n",
            " [  0   0   0   0]]\n",
            "###iteration: 15###\n",
            "train loss= 0.2684, Acc=0.89\n",
            "Test Acc=0.94\n",
            "[[162  10   0   0]\n",
            " [ 44 109   2   0]\n",
            " [  0   0 185   0]\n",
            " [  0   0   0   0]]\n",
            "###iteration: 16###\n",
            "train loss= 0.3270, Acc=0.87\n",
            "Test Acc=0.95\n",
            "[[176  11   1   0]\n",
            " [ 53 110   2   0]\n",
            " [  0   0 159   0]\n",
            " [  0   0   0   0]]\n",
            "###iteration: 17###\n",
            "train loss= 0.2635, Acc=0.89\n",
            "Test Acc=0.96\n",
            "[[165   7   1   0]\n",
            " [ 47 116   0   0]\n",
            " [  0   0 176   0]\n",
            " [  0   0   0   0]]\n",
            "###iteration: 18###\n",
            "train loss= 0.2537, Acc=0.90\n",
            "Test Acc=0.96\n",
            "[[171   3   0   0]\n",
            " [ 48 115   0   0]\n",
            " [  0   0 175   0]\n",
            " [  0   0   0   0]]\n",
            "###iteration: 19###\n",
            "train loss= 0.2467, Acc=0.90\n",
            "Test Acc=0.94\n",
            "[[150   4   0   0]\n",
            " [ 48 127   1   0]\n",
            " [  0   0 182   0]\n",
            " [  0   0   0   0]]\n",
            "###iteration: 20###\n",
            "train loss= 0.3122, Acc=0.87\n",
            "Test Acc=0.90\n",
            "[[167  13   1   0]\n",
            " [ 48 131   3   0]\n",
            " [  0   1 148   0]\n",
            " [  0   0   0   0]]\n",
            "###iteration: 21###\n",
            "train loss= 0.2993, Acc=0.87\n",
            "Test Acc=0.90\n",
            "[[154  13   1   0]\n",
            " [ 52 131   0   0]\n",
            " [  0   0 161   0]\n",
            " [  0   0   0   0]]\n",
            "###iteration: 22###\n",
            "train loss= 0.2661, Acc=0.90\n",
            "Test Acc=0.95\n",
            "[[166   4   1   0]\n",
            " [ 40 125   4   0]\n",
            " [  0   0 172   0]\n",
            " [  0   0   0   0]]\n",
            "###iteration: 23###\n",
            "train loss= 0.2591, Acc=0.89\n",
            "Test Acc=0.97\n",
            "[[161   4   0   0]\n",
            " [ 51 125   0   0]\n",
            " [  0   0 171   0]\n",
            " [  0   0   0   0]]\n",
            "###iteration: 24###\n",
            "train loss= 0.3262, Acc=0.87\n",
            "Test Acc=0.96\n",
            "[[160   3   1   0]\n",
            " [ 56 119   5   0]\n",
            " [  0   0 168   0]\n",
            " [  0   0   0   0]]\n",
            "###iteration: 25###\n",
            "train loss= 0.3358, Acc=0.87\n",
            "Test Acc=0.91\n",
            "[[158  13   1   0]\n",
            " [ 51 117   4   0]\n",
            " [  0   0 168   0]\n",
            " [  0   0   0   0]]\n",
            "###iteration: 26###\n",
            "train loss= 0.2763, Acc=0.89\n",
            "Test Acc=0.88\n",
            "[[148  19   0   0]\n",
            " [ 36 124   2   0]\n",
            " [  1   0 182   0]\n",
            " [  0   0   0   0]]\n",
            "###iteration: 27###\n",
            "train loss= 0.2287, Acc=0.92\n",
            "Test Acc=0.94\n",
            "[[163   9   0   0]\n",
            " [ 30 110   2   0]\n",
            " [  0   1 197   0]\n",
            " [  0   0   0   0]]\n",
            "###iteration: 28###\n",
            "train loss= 0.3602, Acc=0.84\n",
            "Test Acc=0.96\n",
            "[[153   8   1   0]\n",
            " [ 71 100   2   0]\n",
            " [  0   0 177   0]\n",
            " [  0   0   0   0]]\n",
            "###iteration: 29###\n",
            "train loss= 0.3319, Acc=0.85\n",
            "Test Acc=0.97\n",
            "[[191   6   0   0]\n",
            " [ 68  89   1   0]\n",
            " [  0   0 157   0]\n",
            " [  0   0   0   0]]\n",
            "###iteration: 30###\n",
            "train loss= 0.2939, Acc=0.88\n",
            "Test Acc=0.96\n",
            "[[167   3   1   0]\n",
            " [ 56 106   1   0]\n",
            " [  0   0 178   0]\n",
            " [  0   0   0   0]]\n",
            "###iteration: 31###\n",
            "train loss= 0.3362, Acc=0.87\n",
            "Test Acc=0.94\n",
            "[[165  10   2   0]\n",
            " [ 50 118   3   0]\n",
            " [  0   0 164   0]\n",
            " [  0   0   0   0]]\n",
            "###iteration: 32###\n",
            "train loss= 0.2844, Acc=0.88\n",
            "Test Acc=0.92\n",
            "[[166  13   0   0]\n",
            " [ 46 122   1   0]\n",
            " [  0   0 164   0]\n",
            " [  0   0   0   0]]\n",
            "###iteration: 33###\n",
            "train loss= 0.3007, Acc=0.86\n",
            "Test Acc=0.91\n",
            "[[143  16   0   0]\n",
            " [ 53 121   1   0]\n",
            " [  0   0 178   0]\n",
            " [  0   0   0   0]]\n",
            "###iteration: 34###\n",
            "train loss= 0.2698, Acc=0.90\n",
            "Test Acc=0.94\n",
            "[[196   7   1   0]\n",
            " [ 42 118   1   0]\n",
            " [  0   0 147   0]\n",
            " [  0   0   0   0]]\n",
            "###iteration: 35###\n",
            "train loss= 0.2757, Acc=0.88\n",
            "Test Acc=0.95\n",
            "[[148   7   2   0]\n",
            " [ 50 126   3   0]\n",
            " [  0   0 176   0]\n",
            " [  0   0   0   0]]\n",
            "###iteration: 36###\n",
            "train loss= 0.3257, Acc=0.83\n",
            "Test Acc=0.95\n",
            "[[135  13   1   0]\n",
            " [ 71 136   1   0]\n",
            " [  0   0 155   0]\n",
            " [  0   0   0   0]]\n",
            "###iteration: 37###\n",
            "train loss= 0.2919, Acc=0.88\n",
            "Test Acc=0.94\n",
            "[[160   5   1   0]\n",
            " [ 56 112   0   0]\n",
            " [  0   1 177   0]\n",
            " [  0   0   0   0]]\n",
            "###iteration: 38###\n",
            "train loss= 0.2835, Acc=0.88\n",
            "Test Acc=0.93\n",
            "[[145   8   0   0]\n",
            " [ 54 129   1   0]\n",
            " [  0   1 174   0]\n",
            " [  0   0   0   0]]\n",
            "###iteration: 39###\n",
            "train loss= 0.2653, Acc=0.90\n",
            "Test Acc=0.92\n",
            "[[143  11   0   0]\n",
            " [ 34 141   4   0]\n",
            " [  0   0 179   0]\n",
            " [  0   0   0   0]]\n",
            "###iteration: 40###\n",
            "train loss= 0.2687, Acc=0.88\n",
            "Test Acc=0.93\n",
            "[[165  10   0   0]\n",
            " [ 48 113   2   0]\n",
            " [  0   0 174   0]\n",
            " [  0   0   0   0]]\n",
            "###iteration: 41###\n",
            "train loss= 0.2936, Acc=0.88\n",
            "Test Acc=0.95\n",
            "[[156  12   1   0]\n",
            " [ 49 121   1   0]\n",
            " [  0   0 172   0]\n",
            " [  0   0   0   0]]\n",
            "###iteration: 42###\n",
            "train loss= 0.2956, Acc=0.87\n",
            "Test Acc=0.95\n",
            "[[161   7   1   0]\n",
            " [ 58 111   1   0]\n",
            " [  0   0 173   0]\n",
            " [  0   0   0   0]]\n",
            "###iteration: 43###\n",
            "train loss= 0.2632, Acc=0.90\n",
            "Test Acc=0.95\n",
            "[[180   5   1   0]\n",
            " [ 45 106   2   0]\n",
            " [  0   0 173   0]\n",
            " [  0   0   0   0]]\n",
            "###iteration: 44###\n",
            "train loss= 0.2614, Acc=0.89\n",
            "Test Acc=0.93\n",
            "[[153  10   0   0]\n",
            " [ 43 139   3   0]\n",
            " [  0   0 164   0]\n",
            " [  0   0   0   0]]\n",
            "###iteration: 45###\n",
            "train loss= 0.2928, Acc=0.87\n",
            "Test Acc=0.86\n",
            "[[153  17   1   0]\n",
            " [ 49 129   2   0]\n",
            " [  0   0 161   0]\n",
            " [  0   0   0   0]]\n",
            "###iteration: 46###\n",
            "train loss= 0.2870, Acc=0.86\n",
            "Test Acc=0.84\n",
            "[[135  29   0   0]\n",
            " [ 43 130   1   0]\n",
            " [  0   0 174   0]\n",
            " [  0   0   0   0]]\n",
            "###iteration: 47###\n",
            "train loss= 0.2883, Acc=0.87\n",
            "Test Acc=0.92\n",
            "[[168  15   0   0]\n",
            " [ 52 124   1   0]\n",
            " [  0   0 152   0]\n",
            " [  0   0   0   0]]\n",
            "###iteration: 48###\n",
            "train loss= 0.2786, Acc=0.89\n",
            "Test Acc=0.96\n",
            "[[168   6   2   0]\n",
            " [ 49 124   1   0]\n",
            " [  0   0 162   0]\n",
            " [  0   0   0   0]]\n",
            "###iteration: 49###\n",
            "train loss= 0.3020, Acc=0.88\n",
            "Test Acc=0.97\n",
            "[[171   4   1   0]\n",
            " [ 57 129   1   0]\n",
            " [  0   0 149   0]\n",
            " [  0   0   0   0]]\n",
            "###iteration: 50###\n",
            "train loss= 0.2949, Acc=0.88\n",
            "Test Acc=0.96\n",
            "[[172   7   1   0]\n",
            " [ 53 102   1   0]\n",
            " [  0   0 176   0]\n",
            " [  0   0   0   0]]\n",
            "###iteration: 51###\n",
            "train loss= 0.2908, Acc=0.87\n",
            "Test Acc=0.92\n",
            "[[143  12   0   0]\n",
            " [ 56 135   1   0]\n",
            " [  0   0 165   0]\n",
            " [  0   0   0   0]]\n",
            "###iteration: 52###\n",
            "train loss= 0.3030, Acc=0.86\n",
            "Test Acc=0.89\n",
            "[[157  22   0   0]\n",
            " [ 50 123   1   0]\n",
            " [  0   0 159   0]\n",
            " [  0   0   0   0]]\n",
            "###iteration: 53###\n",
            "train loss= 0.2954, Acc=0.89\n",
            "Test Acc=0.92\n",
            "[[151  13   1   0]\n",
            " [ 43 123   0   0]\n",
            " [  0   0 181   0]\n",
            " [  0   0   0   0]]\n",
            "###iteration: 54###\n",
            "train loss= 0.2662, Acc=0.89\n",
            "Test Acc=0.95\n",
            "[[152   7   0   0]\n",
            " [ 48 111   3   0]\n",
            " [  0   0 191   0]\n",
            " [  0   0   0   0]]\n",
            "###iteration: 55###\n",
            "train loss= 0.2670, Acc=0.88\n",
            "Test Acc=0.96\n",
            "[[154   5   1   0]\n",
            " [ 54 107   1   0]\n",
            " [  0   0 190   0]\n",
            " [  0   0   0   0]]\n",
            "###iteration: 56###\n",
            "train loss= 0.3223, Acc=0.86\n",
            "Test Acc=0.97\n",
            "[[197   6   2   0]\n",
            " [ 62 111   1   0]\n",
            " [  0   0 133   0]\n",
            " [  0   0   0   0]]\n",
            "###iteration: 57###\n",
            "train loss= 0.2700, Acc=0.89\n",
            "Test Acc=0.96\n",
            "[[145   6   2   0]\n",
            " [ 48 123   1   0]\n",
            " [  0   0 187   0]\n",
            " [  0   0   0   0]]\n",
            "###iteration: 58###\n",
            "train loss= 0.2744, Acc=0.89\n",
            "Test Acc=0.94\n",
            "[[164   5   0   0]\n",
            " [ 49 136   0   0]\n",
            " [  0   3 155   0]\n",
            " [  0   0   0   0]]\n",
            "###iteration: 59###\n",
            "train loss= 0.2470, Acc=0.90\n",
            "Test Acc=0.92\n",
            "[[161   8   1   0]\n",
            " [ 42 116   0   0]\n",
            " [  0   2 182   0]\n",
            " [  0   0   0   0]]\n",
            "###iteration: 60###\n",
            "train loss= 0.3285, Acc=0.85\n",
            "Test Acc=0.91\n",
            "[[168  15   0   0]\n",
            " [ 58  97   3   0]\n",
            " [  0   0 171   0]\n",
            " [  0   0   0   0]]\n",
            "###iteration: 61###\n",
            "train loss= 0.3001, Acc=0.87\n",
            "Test Acc=0.93\n",
            "[[161  11   0   0]\n",
            " [ 53 120   1   0]\n",
            " [  0   0 166   0]\n",
            " [  0   0   0   0]]\n",
            "###iteration: 62###\n",
            "train loss= 0.2770, Acc=0.89\n",
            "Test Acc=0.95\n",
            "[[164   6   2   0]\n",
            " [ 44 117   3   0]\n",
            " [  0   0 176   0]\n",
            " [  0   0   0   0]]\n",
            "###iteration: 63###\n",
            "train loss= 0.2464, Acc=0.90\n",
            "Test Acc=0.96\n",
            "[[152   6   0   0]\n",
            " [ 43 107   2   0]\n",
            " [  0   0 202   0]\n",
            " [  0   0   0   0]]\n",
            "###iteration: 64###\n",
            "train loss= 0.2724, Acc=0.88\n",
            "Test Acc=0.97\n",
            "[[144   7   0   0]\n",
            " [ 53 117   1   0]\n",
            " [  0   0 190   0]\n",
            " [  0   0   0   0]]\n",
            "###iteration: 65###\n",
            "train loss= 0.2741, Acc=0.88\n",
            "Test Acc=0.96\n",
            "[[145   9   0   0]\n",
            " [ 47 133   3   0]\n",
            " [  0   0 175   0]\n",
            " [  0   0   0   0]]\n",
            "###iteration: 66###\n",
            "train loss= 0.2954, Acc=0.87\n",
            "Test Acc=0.94\n",
            "[[157   9   0   0]\n",
            " [ 57 108   0   0]\n",
            " [  0   0 181   0]\n",
            " [  0   0   0   0]]\n",
            "###iteration: 67###\n",
            "train loss= 0.2895, Acc=0.86\n",
            "Test Acc=0.88\n",
            "[[145  18   0   0]\n",
            " [ 54 113   1   0]\n",
            " [  0   0 181   0]\n",
            " [  0   0   0   0]]\n",
            "###iteration: 68###\n",
            "train loss= 0.2646, Acc=0.89\n",
            "Test Acc=0.93\n",
            "[[171  10   0   0]\n",
            " [ 48 127   0   0]\n",
            " [  0   0 156   0]\n",
            " [  0   0   0   0]]\n",
            "###iteration: 69###\n",
            "train loss= 0.3435, Acc=0.85\n",
            "Test Acc=0.95\n",
            "[[151   7   1   0]\n",
            " [ 66 111   5   0]\n",
            " [  0   0 171   0]\n",
            " [  0   0   0   0]]\n",
            "###iteration: 70###\n",
            "train loss= 0.3113, Acc=0.86\n",
            "Test Acc=0.96\n",
            "[[156   4   1   0]\n",
            " [ 69 111   0   0]\n",
            " [  0   0 171   0]\n",
            " [  0   0   0   0]]\n",
            "###iteration: 71###\n",
            "train loss= 0.2538, Acc=0.89\n",
            "Test Acc=0.97\n",
            "[[143   7   0   0]\n",
            " [ 47 120   0   0]\n",
            " [  0   1 194   0]\n",
            " [  0   0   0   0]]\n",
            "###iteration: 72###\n",
            "train loss= 0.2520, Acc=0.89\n",
            "Test Acc=0.96\n",
            "[[174   2   0   0]\n",
            " [ 53  96   1   0]\n",
            " [  0   0 186   0]\n",
            " [  0   0   0   0]]\n",
            "###iteration: 73###\n",
            "train loss= 0.2819, Acc=0.89\n",
            "Test Acc=0.95\n",
            "[[171   3   2   0]\n",
            " [ 50 116   1   0]\n",
            " [  0   0 169   0]\n",
            " [  0   0   0   0]]\n",
            "###iteration: 74###\n",
            "train loss= 0.2674, Acc=0.88\n",
            "Test Acc=0.93\n",
            "[[132   6   0   0]\n",
            " [ 53 132   1   0]\n",
            " [  0   0 188   0]\n",
            " [  0   0   0   0]]\n",
            "###iteration: 75###\n",
            "train loss= 0.3477, Acc=0.86\n",
            "Test Acc=0.92\n",
            "[[154  17   3   0]\n",
            " [ 48 129   3   0]\n",
            " [  0   0 158   0]\n",
            " [  0   0   0   0]]\n",
            "###iteration: 76###\n",
            "train loss= 0.2799, Acc=0.89\n",
            "Test Acc=0.94\n",
            "[[164   9   1   0]\n",
            " [ 47 118   0   0]\n",
            " [  0   0 173   0]\n",
            " [  0   0   0   0]]\n",
            "###iteration: 77###\n",
            "train loss= 0.2536, Acc=0.90\n",
            "Test Acc=0.96\n",
            "[[157   4   1   0]\n",
            " [ 46 133   0   0]\n",
            " [  0   0 171   0]\n",
            " [  0   0   0   0]]\n",
            "###iteration: 78###\n",
            "train loss= 0.2980, Acc=0.87\n",
            "Test Acc=0.96\n",
            "[[143   4   2   0]\n",
            " [ 61 104   1   0]\n",
            " [  0   0 197   0]\n",
            " [  0   0   0   0]]\n",
            "###iteration: 79###\n",
            "train loss= 0.2696, Acc=0.88\n",
            "Test Acc=0.96\n",
            "[[153   9   0   0]\n",
            " [ 49 135   1   0]\n",
            " [  0   0 165   0]\n",
            " [  0   0   0   0]]\n",
            "###iteration: 80###\n",
            "train loss= 0.2942, Acc=0.88\n",
            "Test Acc=0.96\n",
            "[[167   6   1   0]\n",
            " [ 54 108   1   0]\n",
            " [  0   1 174   0]\n",
            " [  0   0   0   0]]\n",
            "###iteration: 81###\n",
            "train loss= 0.2975, Acc=0.90\n",
            "Test Acc=0.96\n",
            "[[182   5   1   0]\n",
            " [ 44 120   3   0]\n",
            " [  0   0 157   0]\n",
            " [  0   0   0   0]]\n",
            "###iteration: 82###\n",
            "train loss= 0.2991, Acc=0.88\n",
            "Test Acc=0.95\n",
            "[[165   5   2   0]\n",
            " [ 55 118   0   0]\n",
            " [  0   0 167   0]\n",
            " [  0   0   0   0]]\n",
            "###iteration: 83###\n",
            "train loss= 0.3264, Acc=0.88\n",
            "Test Acc=0.94\n",
            "[[166   8   2   0]\n",
            " [ 53 107   1   0]\n",
            " [  0   0 175   0]\n",
            " [  0   0   0   0]]\n",
            "###iteration: 84###\n",
            "train loss= 0.2798, Acc=0.89\n",
            "Test Acc=0.94\n",
            "[[182   7   1   0]\n",
            " [ 45 110   2   0]\n",
            " [  0   0 165   0]\n",
            " [  0   0   0   0]]\n",
            "###iteration: 85###\n",
            "train loss= 0.3136, Acc=0.87\n",
            "Test Acc=0.94\n",
            "[[167  10   0   0]\n",
            " [ 56 107   2   0]\n",
            " [  0   0 170   0]\n",
            " [  0   0   0   0]]\n",
            "###iteration: 86###\n",
            "train loss= 0.3014, Acc=0.88\n",
            "Test Acc=0.95\n",
            "[[149   5   3   0]\n",
            " [ 52 129   3   0]\n",
            " [  0   0 171   0]\n",
            " [  0   0   0   0]]\n",
            "###iteration: 87###\n",
            "train loss= 0.2616, Acc=0.91\n",
            "Test Acc=0.95\n",
            "[[179   2   0   0]\n",
            " [ 43 119   2   0]\n",
            " [  0   0 167   0]\n",
            " [  0   0   0   0]]\n",
            "###iteration: 88###\n",
            "train loss= 0.2917, Acc=0.88\n",
            "Test Acc=0.95\n",
            "[[148   9   1   0]\n",
            " [ 54 115   0   0]\n",
            " [  0   0 185   0]\n",
            " [  0   0   0   0]]\n",
            "###iteration: 89###\n",
            "train loss= 0.2701, Acc=0.89\n",
            "Test Acc=0.95\n",
            "[[138   8   0   0]\n",
            " [ 48 153   2   0]\n",
            " [  0   0 163   0]\n",
            " [  0   0   0   0]]\n",
            "###iteration: 90###\n",
            "train loss= 0.2817, Acc=0.89\n",
            "Test Acc=0.95\n",
            "[[165   7   2   0]\n",
            " [ 46 127   2   0]\n",
            " [  0   0 163   0]\n",
            " [  0   0   0   0]]\n",
            "###iteration: 91###\n",
            "train loss= 0.3246, Acc=0.86\n",
            "Test Acc=0.94\n",
            "[[153   9   0   0]\n",
            " [ 58  95   4   0]\n",
            " [  0   0 193   0]\n",
            " [  0   0   0   0]]\n",
            "###iteration: 92###\n",
            "train loss= 0.2895, Acc=0.88\n",
            "Test Acc=0.95\n",
            "[[164  10   1   0]\n",
            " [ 53 110   0   0]\n",
            " [  0   0 174   0]\n",
            " [  0   0   0   0]]\n",
            "###iteration: 93###\n",
            "train loss= 0.3071, Acc=0.87\n",
            "Test Acc=0.96\n",
            "[[153  12   0   0]\n",
            " [ 52 116   2   0]\n",
            " [  0   0 177   0]\n",
            " [  0   0   0   0]]\n",
            "###iteration: 94###\n",
            "train loss= 0.2586, Acc=0.89\n",
            "Test Acc=0.96\n",
            "[[155   5   0   0]\n",
            " [ 49  98   2   0]\n",
            " [  0   0 203   0]\n",
            " [  0   0   0   0]]\n",
            "###iteration: 95###\n",
            "train loss= 0.3022, Acc=0.86\n",
            "Test Acc=0.96\n",
            "[[152  14   1   0]\n",
            " [ 55 119   1   0]\n",
            " [  0   0 170   0]\n",
            " [  0   0   0   0]]\n",
            "###iteration: 96###\n",
            "train loss= 0.3059, Acc=0.86\n",
            "Test Acc=0.95\n",
            "[[153  12   2   0]\n",
            " [ 53 122   2   0]\n",
            " [  0   1 167   0]\n",
            " [  0   0   0   0]]\n",
            "###iteration: 97###\n",
            "train loss= 0.2851, Acc=0.87\n",
            "Test Acc=0.93\n",
            "[[153  11   0   0]\n",
            " [ 56 122   0   0]\n",
            " [  0   1 169   0]\n",
            " [  0   0   0   0]]\n",
            "###iteration: 98###\n",
            "train loss= 0.2711, Acc=0.91\n",
            "Test Acc=0.94\n",
            "[[159   7   0   0]\n",
            " [ 35 125   4   0]\n",
            " [  0   0 182   0]\n",
            " [  0   0   0   0]]\n",
            "###iteration: 99###\n",
            "train loss= 0.3024, Acc=0.88\n",
            "Test Acc=0.95\n",
            "[[154   8   1   0]\n",
            " [ 51 122   1   0]\n",
            " [  0   0 175   0]\n",
            " [  0   0   0   0]]\n",
            "###iteration: 100###\n",
            "train loss= 0.2962, Acc=0.88\n",
            "Test Acc=0.95\n",
            "[[155   7   0   0]\n",
            " [ 51 115   6   0]\n",
            " [  0   0 178   0]\n",
            " [  0   0   0   0]]\n",
            "###iteration: 101###\n",
            "train loss= 0.2519, Acc=0.89\n",
            "Test Acc=0.95\n",
            "[[144  10   1   0]\n",
            " [ 44 120   0   0]\n",
            " [  0   0 193   0]\n",
            " [  0   0   0   0]]\n",
            "###iteration: 102###\n",
            "train loss= 0.2643, Acc=0.89\n",
            "Test Acc=0.94\n",
            "[[153   8   1   0]\n",
            " [ 42 129   5   0]\n",
            " [  0   0 174   0]\n",
            " [  0   0   0   0]]\n",
            "###iteration: 103###\n",
            "train loss= 0.3216, Acc=0.86\n",
            "Test Acc=0.94\n",
            "[[162  13   0   0]\n",
            " [ 58 112   2   0]\n",
            " [  0   0 165   0]\n",
            " [  0   0   0   0]]\n",
            "###iteration: 104###\n",
            "train loss= 0.2271, Acc=0.91\n",
            "Test Acc=0.94\n",
            "[[161   7   0   0]\n",
            " [ 39 125   1   0]\n",
            " [  0   0 179   0]\n",
            " [  0   0   0   0]]\n",
            "###iteration: 105###\n",
            "train loss= 0.2848, Acc=0.89\n",
            "Test Acc=0.95\n",
            "[[159  11   0   0]\n",
            " [ 42 130   3   0]\n",
            " [  0   0 167   0]\n",
            " [  0   0   0   0]]\n",
            "###iteration: 106###\n",
            "train loss= 0.2514, Acc=0.89\n",
            "Test Acc=0.95\n",
            "[[146   2   0   0]\n",
            " [ 52 116   1   0]\n",
            " [  0   1 194   0]\n",
            " [  0   0   0   0]]\n",
            "###iteration: 107###\n",
            "train loss= 0.3074, Acc=0.88\n",
            "Test Acc=0.94\n",
            "[[154   8   1   0]\n",
            " [ 52 108   0   0]\n",
            " [  0   0 189   0]\n",
            " [  0   0   0   0]]\n",
            "###iteration: 108###\n",
            "train loss= 0.3126, Acc=0.88\n",
            "Test Acc=0.93\n",
            "[[153  11   2   0]\n",
            " [ 45 114   4   0]\n",
            " [  0   0 183   0]\n",
            " [  0   0   0   0]]\n",
            "###iteration: 109###\n",
            "train loss= 0.2584, Acc=0.89\n",
            "Test Acc=0.92\n",
            "[[153   7   0   0]\n",
            " [ 49 128   1   0]\n",
            " [  0   0 174   0]\n",
            " [  0   0   0   0]]\n",
            "###iteration: 110###\n",
            "train loss= 0.2492, Acc=0.90\n",
            "Test Acc=0.95\n",
            "[[172   6   0   0]\n",
            " [ 43 116   0   0]\n",
            " [  0   0 175   0]\n",
            " [  0   0   0   0]]\n",
            "###iteration: 111###\n",
            "train loss= 0.2310, Acc=0.90\n",
            "Test Acc=0.96\n",
            "[[157   7   0   0]\n",
            " [ 44 137   1   0]\n",
            " [  0   0 166   0]\n",
            " [  0   0   0   0]]\n",
            "###iteration: 112###\n",
            "train loss= 0.2838, Acc=0.88\n",
            "Test Acc=0.96\n",
            "[[159  10   2   0]\n",
            " [ 51 119   0   0]\n",
            " [  0   0 171   0]\n",
            " [  0   0   0   0]]\n",
            "###iteration: 113###\n",
            "train loss= 0.2925, Acc=0.88\n",
            "Test Acc=0.96\n",
            "[[150   8   0   0]\n",
            " [ 52 115   4   0]\n",
            " [  0   0 183   0]\n",
            " [  0   0   0   0]]\n",
            "###iteration: 114###\n",
            "train loss= 0.2523, Acc=0.88\n",
            "Test Acc=0.94\n",
            "[[155   8   0   0]\n",
            " [ 52 110   0   0]\n",
            " [  0   0 187   0]\n",
            " [  0   0   0   0]]\n",
            "###iteration: 115###\n",
            "train loss= 0.2876, Acc=0.89\n",
            "Test Acc=0.91\n",
            "[[166  12   0   0]\n",
            " [ 44 120   2   0]\n",
            " [  0   0 168   0]\n",
            " [  0   0   0   0]]\n",
            "###iteration: 116###\n",
            "train loss= 0.2733, Acc=0.89\n",
            "Test Acc=0.93\n",
            "[[161  11   0   0]\n",
            " [ 43 134   1   0]\n",
            " [  0   0 162   0]\n",
            " [  0   0   0   0]]\n",
            "###iteration: 117###\n",
            "train loss= 0.2811, Acc=0.88\n",
            "Test Acc=0.96\n",
            "[[159   7   2   0]\n",
            " [ 52 127   1   0]\n",
            " [  0   0 164   0]\n",
            " [  0   0   0   0]]\n",
            "###iteration: 118###\n",
            "train loss= 0.3406, Acc=0.86\n",
            "Test Acc=0.97\n",
            "[[168   3   1   0]\n",
            " [ 68 108   2   0]\n",
            " [  0   0 162   0]\n",
            " [  0   0   0   0]]\n",
            "###iteration: 119###\n",
            "train loss= 0.2739, Acc=0.88\n",
            "Test Acc=0.97\n",
            "[[153   3   0   0]\n",
            " [ 60 115   0   0]\n",
            " [  0   0 181   0]\n",
            " [  0   0   0   0]]\n",
            "###iteration: 120###\n",
            "train loss= 0.2857, Acc=0.89\n",
            "Test Acc=0.95\n",
            "[[168   6   0   0]\n",
            " [ 49 110   2   0]\n",
            " [  0   0 177   0]\n",
            " [  0   0   0   0]]\n",
            "###iteration: 121###\n",
            "train loss= 0.2645, Acc=0.90\n",
            "Test Acc=0.94\n",
            "[[172   5   0   0]\n",
            " [ 41 121   3   0]\n",
            " [  0   0 170   0]\n",
            " [  0   0   0   0]]\n",
            "###iteration: 122###\n",
            "train loss= 0.2948, Acc=0.88\n",
            "Test Acc=0.95\n",
            "[[162  10   0   0]\n",
            " [ 50 108   2   0]\n",
            " [  0   0 180   0]\n",
            " [  0   0   0   0]]\n",
            "###iteration: 123###\n",
            "train loss= 0.2612, Acc=0.89\n",
            "Test Acc=0.97\n",
            "[[177   8   0   0]\n",
            " [ 45 117   1   0]\n",
            " [  0   0 164   0]\n",
            " [  0   0   0   0]]\n",
            "###iteration: 124###\n",
            "train loss= 0.2806, Acc=0.87\n",
            "Test Acc=0.97\n",
            "[[141   7   0   0]\n",
            " [ 60 112   0   0]\n",
            " [  0   0 192   0]\n",
            " [  0   0   0   0]]\n",
            "###iteration: 125###\n",
            "train loss= 0.2631, Acc=0.88\n",
            "Test Acc=0.97\n",
            "[[163   5   1   0]\n",
            " [ 53 115   1   0]\n",
            " [  0   0 174   0]\n",
            " [  0   0   0   0]]\n",
            "###iteration: 126###\n",
            "train loss= 0.2360, Acc=0.90\n",
            "Test Acc=0.95\n",
            "[[161   6   0   0]\n",
            " [ 41 115   2   0]\n",
            " [  0   0 187   0]\n",
            " [  0   0   0   0]]\n",
            "###iteration: 127###\n",
            "train loss= 0.2545, Acc=0.89\n",
            "Test Acc=0.94\n",
            "[[152  11   0   0]\n",
            " [ 46 122   1   0]\n",
            " [  0   0 180   0]\n",
            " [  0   0   0   0]]\n",
            "###iteration: 128###\n",
            "train loss= 0.2525, Acc=0.89\n",
            "Test Acc=0.92\n",
            "[[162   8   1   0]\n",
            " [ 48 126   0   0]\n",
            " [  0   0 167   0]\n",
            " [  0   0   0   0]]\n",
            "###iteration: 129###\n",
            "train loss= 0.2703, Acc=0.89\n",
            "Test Acc=0.93\n",
            "[[153  11   0   0]\n",
            " [ 46 130   0   0]\n",
            " [  0   0 172   0]\n",
            " [  0   0   0   0]]\n",
            "###iteration: 130###\n",
            "train loss= 0.2486, Acc=0.89\n",
            "Test Acc=0.95\n",
            "[[164   8   0   0]\n",
            " [ 49 120   0   0]\n",
            " [  0   0 171   0]\n",
            " [  0   0   0   0]]\n",
            "###iteration: 131###\n",
            "train loss= 0.2992, Acc=0.86\n",
            "Test Acc=0.96\n",
            "[[153   4   0   0]\n",
            " [ 62 126   3   0]\n",
            " [  0   1 163   0]\n",
            " [  0   0   0   0]]\n",
            "###iteration: 132###\n",
            "train loss= 0.2395, Acc=0.90\n",
            "Test Acc=0.96\n",
            "[[170   4   0   0]\n",
            " [ 49 103   0   0]\n",
            " [  0   0 186   0]\n",
            " [  0   0   0   0]]\n",
            "###iteration: 133###\n",
            "train loss= 0.2849, Acc=0.89\n",
            "Test Acc=0.95\n",
            "[[164   4   1   0]\n",
            " [ 47 116   2   0]\n",
            " [  0   0 178   0]\n",
            " [  0   0   0   0]]\n",
            "###iteration: 134###\n",
            "train loss= 0.3040, Acc=0.88\n",
            "Test Acc=0.94\n",
            "[[167   3   0   0]\n",
            " [ 54 123   3   0]\n",
            " [  0   1 161   0]\n",
            " [  0   0   0   0]]\n",
            "###iteration: 135###\n",
            "train loss= 0.2478, Acc=0.90\n",
            "Test Acc=0.95\n",
            "[[159   8   1   0]\n",
            " [ 44 125   0   0]\n",
            " [  0   0 175   0]\n",
            " [  0   0   0   0]]\n",
            "###iteration: 136###\n",
            "train loss= 0.2354, Acc=0.90\n",
            "Test Acc=0.96\n",
            "[[146   4   0   0]\n",
            " [ 49 118   0   0]\n",
            " [  0   0 195   0]\n",
            " [  0   0   0   0]]\n",
            "###iteration: 137###\n",
            "train loss= 0.2487, Acc=0.90\n",
            "Test Acc=0.96\n",
            "[[163   3   0   0]\n",
            " [ 47 113   1   0]\n",
            " [  0   0 185   0]\n",
            " [  0   0   0   0]]\n",
            "###iteration: 138###\n",
            "train loss= 0.2835, Acc=0.88\n",
            "Test Acc=0.95\n",
            "[[166   4   0   0]\n",
            " [ 55 119   1   0]\n",
            " [  0   0 167   0]\n",
            " [  0   0   0   0]]\n",
            "###iteration: 139###\n",
            "train loss= 0.2603, Acc=0.89\n",
            "Test Acc=0.95\n",
            "[[160  12   0   0]\n",
            " [ 45 130   1   0]\n",
            " [  0   0 164   0]\n",
            " [  0   0   0   0]]\n",
            "###iteration: 140###\n",
            "train loss= 0.2602, Acc=0.89\n",
            "Test Acc=0.95\n",
            "[[148  11   0   0]\n",
            " [ 46 126   1   0]\n",
            " [  0   0 180   0]\n",
            " [  0   0   0   0]]\n",
            "###iteration: 141###\n",
            "train loss= 0.2699, Acc=0.89\n",
            "Test Acc=0.96\n",
            "[[165  10   0   0]\n",
            " [ 46 117   2   0]\n",
            " [  0   0 172   0]\n",
            " [  0   0   0   0]]\n",
            "###iteration: 142###\n",
            "train loss= 0.3023, Acc=0.88\n",
            "Test Acc=0.96\n",
            "[[161   8   0   0]\n",
            " [ 51 122   2   0]\n",
            " [  0   1 167   0]\n",
            " [  0   0   0   0]]\n",
            "###iteration: 143###\n",
            "train loss= 0.2994, Acc=0.88\n",
            "Test Acc=0.96\n",
            "[[164   8   2   0]\n",
            " [ 53 124   0   0]\n",
            " [  0   0 161   0]\n",
            " [  0   0   0   0]]\n",
            "###iteration: 144###\n",
            "train loss= 0.3048, Acc=0.87\n",
            "Test Acc=0.96\n",
            "[[163   8   2   0]\n",
            " [ 54 119   0   0]\n",
            " [  0   1 165   0]\n",
            " [  0   0   0   0]]\n",
            "###iteration: 145###\n",
            "train loss= 0.2382, Acc=0.90\n",
            "Test Acc=0.95\n",
            "[[155   7   1   0]\n",
            " [ 42 131   0   0]\n",
            " [  0   0 176   0]\n",
            " [  0   0   0   0]]\n",
            "###iteration: 146###\n",
            "train loss= 0.2683, Acc=0.90\n",
            "Test Acc=0.95\n",
            "[[171   8   2   0]\n",
            " [ 41 142   0   0]\n",
            " [  0   0 148   0]\n",
            " [  0   0   0   0]]\n",
            "###iteration: 147###\n",
            "train loss= 0.2699, Acc=0.88\n",
            "Test Acc=0.95\n",
            "[[158   6   0   0]\n",
            " [ 51 117   2   0]\n",
            " [  0   0 178   0]\n",
            " [  0   0   0   0]]\n",
            "###iteration: 148###\n",
            "train loss= 0.2998, Acc=0.88\n",
            "Test Acc=0.95\n",
            "[[152   8   0   0]\n",
            " [ 54 135   2   0]\n",
            " [  0   0 161   0]\n",
            " [  0   0   0   0]]\n",
            "###iteration: 149###\n",
            "train loss= 0.2623, Acc=0.90\n",
            "Test Acc=0.95\n",
            "[[166   4   0   0]\n",
            " [ 47 124   2   0]\n",
            " [  0   0 169   0]\n",
            " [  0   0   0   0]]\n",
            "###iteration: 150###\n",
            "train loss= 0.2768, Acc=0.88\n",
            "Test Acc=0.95\n",
            "[[171   6   0   0]\n",
            " [ 52 133   1   0]\n",
            " [  0   0 149   0]\n",
            " [  0   0   0   0]]\n",
            "###iteration: 151###\n",
            "train loss= 0.2312, Acc=0.90\n",
            "Test Acc=0.96\n",
            "[[162   4   0   0]\n",
            " [ 47 115   0   0]\n",
            " [  0   0 184   0]\n",
            " [  0   0   0   0]]\n",
            "###iteration: 152###\n",
            "train loss= 0.2357, Acc=0.90\n",
            "Test Acc=0.96\n",
            "[[166   7   0   0]\n",
            " [ 45 109   1   0]\n",
            " [  0   0 184   0]\n",
            " [  0   0   0   0]]\n",
            "###iteration: 153###\n",
            "train loss= 0.2599, Acc=0.89\n",
            "Test Acc=0.96\n",
            "[[165   6   0   0]\n",
            " [ 50 116   0   0]\n",
            " [  0   0 175   0]\n",
            " [  0   0   0   0]]\n",
            "###iteration: 154###\n",
            "train loss= 0.2897, Acc=0.87\n",
            "Test Acc=0.96\n",
            "[[152   8   0   0]\n",
            " [ 57 115   1   0]\n",
            " [  0   0 179   0]\n",
            " [  0   0   0   0]]\n",
            "###iteration: 155###\n",
            "train loss= 0.2669, Acc=0.89\n",
            "Test Acc=0.95\n",
            "[[168   5   0   0]\n",
            " [ 51 111   2   0]\n",
            " [  0   0 175   0]\n",
            " [  0   0   0   0]]\n",
            "###iteration: 156###\n",
            "train loss= 0.2516, Acc=0.90\n",
            "Test Acc=0.94\n",
            "[[145  12   0   0]\n",
            " [ 36 134   3   0]\n",
            " [  0   0 182   0]\n",
            " [  0   0   0   0]]\n",
            "###iteration: 157###\n",
            "train loss= 0.2727, Acc=0.88\n",
            "Test Acc=0.95\n",
            "[[148   9   0   0]\n",
            " [ 51 108   3   0]\n",
            " [  0   0 193   0]\n",
            " [  0   0   0   0]]\n",
            "###iteration: 158###\n",
            "train loss= 0.2772, Acc=0.89\n",
            "Test Acc=0.95\n",
            "[[166   9   1   0]\n",
            " [ 48 114   0   0]\n",
            " [  0   0 174   0]\n",
            " [  0   0   0   0]]\n",
            "###iteration: 159###\n",
            "train loss= 0.2551, Acc=0.89\n",
            "Test Acc=0.96\n",
            "[[180   6   0   0]\n",
            " [ 47 136   1   0]\n",
            " [  0   0 142   0]\n",
            " [  0   0   0   0]]\n",
            "###iteration: 160###\n",
            "train loss= 0.2497, Acc=0.90\n",
            "Test Acc=0.96\n",
            "[[160   7   1   0]\n",
            " [ 45 128   0   0]\n",
            " [  0   0 171   0]\n",
            " [  0   0   0   0]]\n",
            "###iteration: 161###\n",
            "train loss= 0.2719, Acc=0.89\n",
            "Test Acc=0.95\n",
            "[[156   5   0   0]\n",
            " [ 53 130   0   0]\n",
            " [  0   0 168   0]\n",
            " [  0   0   0   0]]\n",
            "###iteration: 162###\n",
            "train loss= 0.2678, Acc=0.88\n",
            "Test Acc=0.91\n",
            "[[147   8   0   0]\n",
            " [ 52 124   1   0]\n",
            " [  0   1 179   0]\n",
            " [  0   0   0   0]]\n",
            "###iteration: 163###\n",
            "train loss= 0.3237, Acc=0.84\n",
            "Test Acc=0.90\n",
            "[[153  17   1   0]\n",
            " [ 62 119   1   0]\n",
            " [  0   0 159   0]\n",
            " [  0   0   0   0]]\n",
            "###iteration: 164###\n",
            "train loss= 0.2757, Acc=0.90\n",
            "Test Acc=0.95\n",
            "[[175   5   0   0]\n",
            " [ 45 113   3   0]\n",
            " [  0   0 171   0]\n",
            " [  0   0   0   0]]\n",
            "###iteration: 165###\n",
            "train loss= 0.2799, Acc=0.88\n",
            "Test Acc=0.96\n",
            "[[159   5   0   0]\n",
            " [ 57 115   1   0]\n",
            " [  0   0 175   0]\n",
            " [  0   0   0   0]]\n",
            "###iteration: 166###\n",
            "train loss= 0.2871, Acc=0.87\n",
            "Test Acc=0.97\n",
            "[[175   3   1   0]\n",
            " [ 60 127   1   0]\n",
            " [  0   0 145   0]\n",
            " [  0   0   0   0]]\n",
            "###iteration: 167###\n",
            "train loss= 0.3037, Acc=0.89\n",
            "Test Acc=0.97\n",
            "[[184   8   1   0]\n",
            " [ 48 112   1   0]\n",
            " [  0   0 158   0]\n",
            " [  0   0   0   0]]\n",
            "###iteration: 168###\n",
            "train loss= 0.2616, Acc=0.90\n",
            "Test Acc=0.96\n",
            "[[184   5   0   0]\n",
            " [ 47 108   0   0]\n",
            " [  0   0 168   0]\n",
            " [  0   0   0   0]]\n",
            "###iteration: 169###\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "eW3jArXXGui4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "sess.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "3PUIfuv42S2p",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Generate Predictions for the raw data by batch\n",
        "batch_size = 64\n",
        "batch_num = len(dataX)//batch_size\n",
        "nn_dataX = None\n",
        "print(batch_num)\n",
        "#restore rnn session\n",
        "sess = tf.Session()\n",
        "saver.restore(sess, 'my_net/save_net_rnn.ckpt')\n",
        "for i in range(0,batch_num+1):\n",
        "  print(i)\n",
        "  if(i!=batch_num):\n",
        "      batch_X = dataX[i*batch_size:(i+1)*batch_size,]\n",
        "      batch_X = scaler.transform(batch_X).reshape([-1,window_size,1])\n",
        "      batch_pred = sess.run(pred,feed_dict={tf_x: batch_X})\n",
        "      if nn_dataX is None:\n",
        "        nn_dataX = batch_pred\n",
        "      else:\n",
        "        nn_dataX = np.vstack([nn_dataX,batch_pred])\n",
        "      \n",
        "  else: \n",
        "      batch_X = dataX[i*batch_size:]\n",
        "      batch_X = scaler.transform(batch_X).reshape([-1,window_size,1])\n",
        "      batch_pred = sess.run(pred,feed_dict={tf_x: batch_X})\n",
        "      nn_dataX = np.vstack([nn_dataX,batch_pred])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "J80clWecJ2i-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "outputId": "32bd357d-dbbe-45d3-9e20-d4e1a9ab0661"
      },
      "cell_type": "code",
      "source": [
        "y_lables_argmax = np.argmax(cdeY_1hot,1)\n",
        "y_pred_argmax = np.argmax(nn_dataX,1)\n",
        "confusion = tf.confusion_matrix(labels=y_lables_argmax, predictions=y_pred_argmax, num_classes=4)\n",
        "#print('Confusion Matrix: \\n\\n', tf.Tensor.eval(confusion,feed_dict=None))\n",
        "print(confusion.eval(session=sess))"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[975728  46305   4758      0]\n",
            " [  2957   7040     98      0]\n",
            " [     0      0  11216      0]\n",
            " [     2      6     31      0]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "dGpUVu4FH5Np",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "result_df = pd.DataFrame(data=nn_dataX)\n",
        "result_df.to_csv('nn_data.csv',index=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "A8zsoFtYLmDv",
        "colab_type": "code",
        "outputId": "336f2238-22d5-4479-cb1f-83b66becf147",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        }
      },
      "cell_type": "code",
      "source": [
        "df = pd.Series(y_pred_argmax)\n",
        "df.value_counts()"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    978687\n",
              "1     53351\n",
              "2     16103\n",
              "dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "metadata": {
        "id": "-snfquS5MvMu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 762
        },
        "outputId": "9d17f68a-49b6-46f0-9699-15ca2a8750b6"
      },
      "cell_type": "code",
      "source": [
        "whe_datetime = pd.read_csv('Electricity_WHE.csv')['unix_ts']\n",
        "whe_datetime = pd.to_datetime(whe_datetime,unit='s')\n",
        "whe_datetime['datetime'] = whe_datetime[window_size-1:]\n",
        "whe_datetime['month'] = whe_dt['datetime'].dt.month\n",
        "whe_datetime['hour'] = WHE_data['datetime'].dt.hour\n",
        "whe_datetime['weekday'] = WHE_data['datetime'].dt.dayofweek"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-46-08da90085e89>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mwhe_datetime\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Electricity_WHE.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'unix_ts'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mwhe_datetime\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_datetime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwhe_datetime\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0munit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m's'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mwhe_datetime\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'datetime'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwhe_datetime\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mwindow_size\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mwhe_datetime\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'month'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwhe_dt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'datetime'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmonth\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mwhe_datetime\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'hour'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mWHE_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'datetime'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhour\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/series.py\u001b[0m in \u001b[0;36m__setitem__\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m    791\u001b[0m         \u001b[0;31m# do the setitem\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    792\u001b[0m         \u001b[0mcacher_needs_updating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_is_chained_assignment_possible\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 793\u001b[0;31m         \u001b[0msetitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    794\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcacher_needs_updating\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    795\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_update_cacher\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/series.py\u001b[0m in \u001b[0;36msetitem\u001b[0;34m(key, value)\u001b[0m\n\u001b[1;32m    787\u001b[0m                     \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    788\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 789\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_with\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    790\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    791\u001b[0m         \u001b[0;31m# do the setitem\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/series.py\u001b[0m in \u001b[0;36m_set_with\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m    835\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbool_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    836\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 837\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_labels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    838\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    839\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_set_labels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/series.py\u001b[0m in \u001b[0;36m_set_labels\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m    845\u001b[0m         \u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindexer\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    846\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 847\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'%s not contained in the index'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    848\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    849\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: ['d' 'a' 't' 'e' 't' 'i' 'm' 'e'] not contained in the index"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "cPCVXOAGr2St",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#define the Feedforward Model\n",
        "'''Feedforward Model Definition'''\n",
        "tf.reset_default_graph()\n",
        "''''''\n",
        "#define inputs\n",
        "tf_x = tf.placeholder(tf.float32, [None, window_size,47],name='ff_x')\n",
        "tf_y = tf.placeholder(tf.int32, [None, 4],name='ff_y')\n",
        "\n",
        "l1 = tf.layers.dense(tf_x,64,activation=tf.nn.leaky_relu,name='ff_l1')\n",
        "l2 = tf.layers.dense(l1,128,activation=tf.nn.leaky_relu,name='ff_l2')\n",
        "l3 = tf.layers.dense(l2,256,activation=tf.nn.leaky_relu,name='ff_l3')\n",
        "l4 = tf.layers.dense(l3,128,activation=tf.nn.leaky_relu,name='ff_l4')\n",
        "l5 = tf.layers.dense(l4,64,activation=tf.nn.leaky_relu,name='ff_l5')\n",
        "l6 = tf.layers.dense(l5,32,activation=tf.nn.leaky_relu,name='ff_l6')\n",
        "l7 = tf.layers.dense(l6,16,activation=tf.nn.leaky_relu,name='ff_l7')\n",
        "l8 = tf.layers.dense(l7,8,activation=tf.nn.leaky_relu,name='ff_l8')\n",
        "pred = tf.layers.dense(l8,4,activation=tf.nn.relu,name='ff_pred')\n",
        "\n",
        "with tf.name_scope('loss'):\n",
        "    cross_entropy =  tf.nn.softmax_cross_entropy_with_logits_v2(labels=tf_y, logits=pred) \n",
        "    loss = tf.reduce_mean(cross_entropy)\n",
        "    tf.summary.scalar(\"loss\",tensor=loss)\n",
        "\n",
        "train_op = tf.train.AdamOptimizer(LR).minimize(loss)\n",
        "\n",
        "accuracy = tf.reduce_mean(tf.cast(tf.equal(tf.argmax(tf_y, axis=1), tf.argmax(pred, axis=1)), tf.float32))\n",
        "\n",
        "init_op = tf.group(tf.global_variables_initializer(), tf.local_variables_initializer()) \n",
        "saver = tf.train.Saver()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xpBZbZNYeY34",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#!mkdir my_net\n",
        "mv checkpoint save_net_rnn.ckpt.data-00000-of-00001 save_net_rnn.ckpt.index save_net_rnn.ckpt.meta my_net"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}
