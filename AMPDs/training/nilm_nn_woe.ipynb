{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "nilm_nn_woe.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RhysWangJunfei/nilm/blob/master/AMPDs/training/nilm_nn_woe.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y27H1CTRw_5h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import io\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split \n",
        "from google.colab import files\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mUTm0K0AxC1R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#uploaded = files.upload()\n",
        "'''Load data'''\n",
        "#non_time_data = pd.read_csv(io.BytesIO(uploaded['Electricity_WHE.csv']))[['unix_ts']]\n",
        "non_time_data = pd.read_csv('Electricity_WHE.csv')['unix_ts']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-yvZYgV4xIYZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#uploaded = files.upload()\n",
        "#CDE_data = pd.read_csv(io.BytesIO(uploaded['Electricity_CDE.csv']))['P']\n",
        "WOE_data = pd.read_csv('Electricity_WOE.csv')['P']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hwbcj0RRxI9q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#uploaded = files.upload()\n",
        "#nn_data = pd.read_csv(io.BytesIO(uploaded['nn_data.csv']))\n",
        "nn_data = pd.read_csv('nn_data.csv')\n",
        "print(nn_data.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ogxmIrZSxK8s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "window_size=60\n",
        "non_time_data = pd.to_datetime(non_time_data,unit='s')\n",
        "whe_month = non_time_data.dt.month.as_matrix()[window_size-1:].reshape([-1,1])\n",
        "whe_hour = non_time_data.dt.hour.as_matrix()[window_size-1:].reshape([-1,1])\n",
        "whe_weekday = non_time_data.dt.dayofweek.as_matrix()[window_size-1:].reshape([-1,1])\n",
        "non_time = np.hstack([whe_month,whe_hour,whe_weekday])\n",
        "non_time = np.concatenate([non_time[0:472500,:],non_time[475500:,:]],axis=0)\n",
        "\n",
        "nn_data_ = np.concatenate([nn_data[0:472500],nn_data[475500:]],axis=0)\n",
        "\n",
        "data_arr = np.hstack([nn_data_,non_time])\n",
        "\n",
        "dataX = pd.get_dummies(pd.DataFrame(data_arr),columns=[3,4,5]).values\n",
        "woe_Y = WOE_data.as_matrix()[window_size-1:].reshape([-1,1])\n",
        "woe_Y = np.concatenate([woe_Y[0:472500,:],woe_Y[475500:,:]],axis=0)\n",
        "categorized_woe_Y = np.ones(app_Y.shape)*1\n",
        "categorized_woe_Y[[np.where(app_Y<800)[0]],:]=0\n",
        "encoder = OneHotEncoder(handle_unknown='ignore',sparse=False)\n",
        "cdeY_1hot = encoder.fit_transform(categorized_cde_Y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PBk5nRCPxxtj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train_cv, X_test, y_train_cv, y_test = train_test_split(\\\n",
        "            dataX,cdeY_1hot,\\\n",
        "            test_size=0.02, shuffle=True)\n",
        "X_train, X_cv, y_train, y_cv = train_test_split(X_train_cv, y_train_cv, test_size=0.02, shuffle=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZnCRrI1GxtuQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "max_indice = np.argmax(y_train,1)\n",
        "df = pd.Series(max_indice)\n",
        "df.value_counts()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ES_M3MZrxutY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "tl = RandomUnderSampler(sampling_strategy={0:9000,1:9000,2:9000})\n",
        "X_train, y_train = tl.fit_resample(X_train, y_train)\n",
        "y_train=encoder.transform(y_train)\n",
        "whole_train = np.concatenate([X_train,y_train],axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sndrjoWox4HE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''Hyper parameters for deep learning'''\n",
        "# Hyper Parameters\n",
        "LR = 0.001               # learning rate\n",
        "#cfg_list = nf.model_configs()\n",
        "#error_list = []\n",
        "\n",
        "#hyperparameters\n",
        "batch_size=128\n",
        "unit_num=128"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_QvlRWGKx5Jo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#define the Feedforward Model\n",
        "'''Feedforward Model Definition'''\n",
        "tf.reset_default_graph()\n",
        "\n",
        "''''''\n",
        "#define inputs\n",
        "tf_x = tf.placeholder(tf.float32, [None, 45],name='ff_x')\n",
        "tf_y = tf.placeholder(tf.int32, [None, 2],name='ff_y')\n",
        "\n",
        "l1 = tf.layers.dense(tf_x,64,activation=tf.nn.leaky_relu,name='ff_l1')\n",
        "l2 = tf.layers.dense(l1,128,activation=tf.nn.leaky_relu,name='ff_l2')\n",
        "l3 = tf.layers.dense(l2,256,activation=tf.nn.leaky_relu,name='ff_l3')\n",
        "l4 = tf.layers.dense(l3,128,activation=tf.nn.leaky_relu,name='ff_l4')\n",
        "l5 = tf.layers.dense(l4,64,activation=tf.nn.leaky_relu,name='ff_l5')\n",
        "l6 = tf.layers.dense(l5,32,activation=tf.nn.leaky_relu,name='ff_l6')\n",
        "l7 = tf.layers.dense(l6,8,activation=tf.nn.leaky_relu,name='ff_l7')\n",
        "l8 = tf.layers.dense(l7,4,activation=tf.nn.leaky_relu,name='ff_l8')\n",
        "pred = tf.layers.dense(l8,2,activation=tf.nn.relu,name='ff_pred')\n",
        "\n",
        "with tf.name_scope('loss'):\n",
        "    cross_entropy =  tf.nn.softmax_cross_entropy_with_logits_v2(labels=tf_y, logits=pred) \n",
        "    loss = tf.reduce_mean(cross_entropy)\n",
        "    tf.summary.scalar(\"loss\",tensor=loss)\n",
        "\n",
        "train_op = tf.train.AdamOptimizer(LR).minimize(loss)\n",
        "\n",
        "accuracy = tf.reduce_mean(tf.cast(tf.equal(tf.argmax(tf_y, axis=1), tf.argmax(pred, axis=1)), tf.float32))\n",
        "\n",
        "init_op = tf.group(tf.global_variables_initializer(), tf.local_variables_initializer()) \n",
        "saver = tf.train.Saver()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A0fsqmn5x702",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sess = tf.Session()\n",
        "#sess.run(init_op)\n",
        "saver.restore(sess, 'my_net/save_rnn.ckpt')\n",
        "\n",
        "for j in range(0,3000):\n",
        "    print('###iteration: '+str(j)+'###')\n",
        "    batch_index = np.random.choice(27000,batch_size)\n",
        "    batch_train = whole_train[batch_index,:]\n",
        "    batch_X = batch_train[:,:-2].reshape([-1,45])\n",
        "    batch_y = batch_train[:,-2:]\n",
        "    sess.run(train_op,{tf_x:batch_X , tf_y:batch_y})\n",
        "    cost_ = sess.run(loss,{tf_x:batch_X, tf_y:batch_y})\n",
        "    print('train loss= %.4f' % cost_)\n",
        "    if(j%99==0):\n",
        "      acc_train = sess.run(accuracy,{tf_x:batch_X, tf_y:batch_y})\n",
        "      print('train loss= %.4f' % cost_+', Acc=%.2f'% acc_train)\n",
        "      acc_cv = sess.run(accuracy,{tf_x: X_cv.reshape([-1,45]), tf_y:y_cv})\n",
        "      print('CV Acc=%.2f'% acc_cv)\n",
        "      pre_cv = sess.run(pred,feed_dict={tf_x: X_cv.reshape([-1,45]), tf_y:y_cv})\n",
        "      y_lables_argmax = tf.argmax(y_cv,axis=1)  \n",
        "      y_pred_cv_argmax = tf.argmax(pre_cv,axis=1)\n",
        "      confusion = tf.confusion_matrix(labels=y_lables_argmax, predictions=y_pred_cv_argmax, num_classes=3)\n",
        "      print(confusion.eval(session=sess,feed_dict={tf_x:X_cv.reshape([-1,45]), tf_y:y_cv}))\n",
        "      save_path = saver.save(sess, \"my_net/save_rnn.ckpt\")\n",
        "pre = sess.run(pred,feed_dict={tf_x: X_test.reshape([X_test.shape[0],45]), tf_y: y_test})\n",
        "y_lables_argmax = np.argmax(y_test,1)\n",
        "y_pred_argmax = np.argmax(pre,1)\n",
        "confusion = tf.confusion_matrix(labels=y_lables_argmax, predictions=y_pred_argmax, num_classes=3)\n",
        "#print('Confusion Matrix: \\n\\n', tf.Tensor.eval(confusion,feed_dict=None))\n",
        "print(confusion.eval(session=sess))\n",
        "sess.close()\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}