{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "nilm_rnn_woe.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RhysWangJunfei/nilm/blob/master/AMPDs/training/nilm_rnn_woe.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "FMCNNZrQp0Sv",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import io\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split \n",
        "from google.colab import files\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "y4bJENdUqFjy",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "'''Sliding window function'''\n",
        "def create_dataset(dataset, look_back=1):\n",
        "    dataX = []\n",
        "    for i in range(len(dataset)-look_back+1):\n",
        "        a = dataset[i:(i+look_back)]\n",
        "        dataX.append(a)\n",
        "    return np.array(dataX)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "PVqguOh4qHNm",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "WHE_data = pd.read_csv('Electricity_WHE.csv')['P']\n",
        "WOE_data = pd.read_csv('Electricity_WOE.csv')['P']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "KkOdH_prqKy-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "outputId": "f679ce40-1e21-4611-fb2f-67275763193a"
      },
      "cell_type": "code",
      "source": [
        "window_size=60\n",
        "\n",
        "dataX_raw = create_dataset(WHE_data.as_matrix(), window_size)\n",
        "\n",
        "app_Y_raw = WOE_data[window_size-1:].values.reshape([WOE_data.shape[0]-window_size+1,1])\n",
        "dataX = np.concatenate([dataX_raw[0:472500,:],dataX_raw[475500:,:]],axis=0)\n",
        "app_Y = np.concatenate([app_Y_raw[0:472500,:],app_Y_raw[475500:,:]],axis=0)\n",
        "categorized_Y = np.ones(app_Y.shape)*1\n",
        "categorized_Y[[np.where(app_Y<800)[0]],:]=0\n",
        "\n",
        "encoder = OneHotEncoder(handle_unknown='ignore',sparse=False)\n",
        "Y_1hot = encoder.fit_transform(categorized_Y)\n",
        "\n",
        "dataX_train_cv = np.concatenate([dataX[0:450000,:],dataX[470000:,:]],axis=0)\n",
        "dataX_test = dataX[450000:470000,:]\n",
        "Y_1hot_train_cv = np.concatenate([Y_1hot[0:450000,:],Y_1hot[470000:,:]],axis=0)\n",
        "Y_1hot_test = Y_1hot[450000:470000,:]\n",
        "\n",
        "X_train, X_cv, y_train, y_cv = train_test_split(dataX_train_cv, Y_1hot_train_cv, test_size=0.01, shuffle=True)\n",
        "scaler = MinMaxScaler()\n",
        "scaler.fit(X_train.astype(float))\n",
        "X_train = scaler.transform(X_train)\n",
        "X_cv = scaler.transform(X_cv)\n",
        "X_test = scaler.transform(dataX_test)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:3: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
            "  This is separate from the ipykernel package so we can avoid doing imports until\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "32agK7a1qVNQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "b21ab653-8b54-4a79-e068-99f74bff3305"
      },
      "cell_type": "code",
      "source": [
        "max_indice = np.argmax(Y_1hot,1)\n",
        "df = pd.Series(max_indice)\n",
        "df.value_counts()"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    1045658\n",
              "1       2483\n",
              "dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "metadata": {
        "id": "5giG1X3rymx3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "'''Hyper parameters for deep learning'''\n",
        "# Hyper Parameters\n",
        "LR = 0.001               # learning rate\n",
        "#cfg_list = nf.model_configs()\n",
        "#error_list = []\n",
        "\n",
        "#hyperparameters\n",
        "batch_size=512\n",
        "num_units = [256, 256]\n",
        "\n",
        "'''RNN Model Definition'''\n",
        "tf.reset_default_graph()\n",
        "''''''\n",
        "#define inputs\n",
        "tf_x = tf.placeholder(tf.float32, [None, window_size,1],name='x')\n",
        "tf_y = tf.placeholder(tf.int32, [None, 2],name='y')\n",
        "\n",
        "cells = [tf.keras.layers.LSTMCell(units=n) for n in num_units]\n",
        "stacked_rnn_cell = tf.keras.layers.StackedRNNCells(cells)\n",
        "outputs, (h_c, h_n) = tf.nn.dynamic_rnn(\n",
        "        stacked_rnn_cell,                   # cell you have chosen\n",
        "        tf_x,                      # input\n",
        "        initial_state=None,         # the initial hidden state\n",
        "        dtype=tf.float32,           # must given if set initial_state = None\n",
        "        time_major=False,           # False: (batch, time step, input); True: (time step, batch, input)\n",
        ")\n",
        "l1 = tf.layers.dense(outputs[:, -1, :],256,activation=tf.nn.leaky_relu,name='l1')\n",
        "l2 = tf.layers.dense(l1,128,activation=tf.nn.leaky_relu,name='l2')\n",
        "l3 = tf.layers.dense(l2,64,activation=tf.nn.leaky_relu,name='l3')\n",
        "l4 = tf.layers.dense(l3,32,activation=tf.nn.leaky_relu,name='l4')\n",
        "l5 = tf.layers.dense(l4,16,activation=tf.nn.leaky_relu,name='l5')\n",
        "l6 = tf.layers.dense(l5,8,activation=tf.nn.leaky_relu,name='l6')\n",
        "l7 = tf.layers.dense(l6,4,activation=tf.nn.leaky_relu,name='l7')\n",
        "pred = tf.layers.dense(l4,2,activation=tf.nn.relu,name='pred')\n",
        "\n",
        "with tf.name_scope('loss'):\n",
        "    cross_entropy =  tf.nn.softmax_cross_entropy_with_logits_v2(labels=tf_y, logits=pred) \n",
        "    loss = tf.reduce_mean(cross_entropy)\n",
        "    tf.summary.scalar(\"loss\",tensor=loss)\n",
        "\n",
        "train_op = tf.train.AdamOptimizer(LR).minimize(loss)\n",
        "\n",
        "accuracy = tf.reduce_mean(tf.cast(tf.equal(tf.argmax(tf_y, axis=1), tf.argmax(pred, axis=1)), tf.float32))\n",
        "\n",
        "init_op = tf.group(tf.global_variables_initializer(), tf.local_variables_initializer()) \n",
        "saver = tf.train.Saver()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GIN0Zz-BrrLZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 257
        },
        "outputId": "3b63009e-78d5-4d48-ed12-f0fa958828a2"
      },
      "cell_type": "code",
      "source": [
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "tl = RandomUnderSampler(sampling_strategy={0:2400,1:2400})\n",
        "\n",
        "sess = tf.Session()\n",
        "#sess.run(init_op)\n",
        "saver.restore(sess, 'my_net/save_net_rnn.ckpt')\n",
        "\n",
        "for i in range(0,1):\n",
        "    X, y_1 = tl.fit_resample(X_train, y_train)\n",
        "    y=encoder.transform(y_1)\n",
        "    whole_train = np.concatenate([X,y],axis=1)\n",
        "    for j in range(0,1):\n",
        "        print('Loop'+str(i)+',###iteration: '+str(j)+'###')\n",
        "        batch_index = np.random.choice(4800,batch_size)\n",
        "        batch_train = whole_train[batch_index,:]\n",
        "        batch_X = batch_train[:,:-2].reshape([batch_size,window_size,1])\n",
        "        batch_y = batch_train[:,-2:]\n",
        "        sess.run(train_op,{tf_x:batch_X , tf_y:batch_y})\n",
        "        cost_ = sess.run(loss,{tf_x:batch_X, tf_y:batch_y})\n",
        "        print('train loss= %.4f' % cost_)\n",
        "        if(j%99==0):\n",
        "            acc_train = sess.run(accuracy,{tf_x:batch_X, tf_y:batch_y})\n",
        "            acc_cv = sess.run(accuracy,feed_dict={tf_x: X_cv.reshape([X_cv.shape[0],window_size,1]), tf_y:y_cv})\n",
        "            print('train loss= %.4f' % cost_+', Acc=%.2f'% acc_train)\n",
        "            print('Test Acc=%.2f'% acc_cv)\n",
        "            pre = sess.run(pred,feed_dict={tf_x: X_cv.reshape([-1,window_size,1]), tf_y:y_cv})\n",
        "            y_lables_argmax = tf.argmax(y_cv,axis=1)  \n",
        "            y_pred_argmax = tf.argmax(pre,axis=1)\n",
        "            confusion = tf.confusion_matrix(labels=y_lables_argmax, predictions=y_pred_argmax, num_classes=2)\n",
        "            print(confusion.eval(session=sess,feed_dict={tf_x: batch_X, tf_y:batch_y}))\n",
        "            save_path = saver.save(sess, \"my_net/save_net_rnn.ckpt\")\n",
        "#pre = sess.run(pred,feed_dict={tf_x: X_test.reshape([X_test.shape[0],window_size,1]), tf_y: y_test})\n",
        "#y_lables_argmax = np.argmax(y_test,1)\n",
        "#y_pred_argmax = np.argmax(pre,1)\n",
        "#confusion = tf.confusion_matrix(labels=y_lables_argmax, predictions=y_pred_argmax, num_classes=2)\n",
        "#print('Confusion Matrix: \\n\\n', tf.Tensor.eval(confusion,feed_dict=None))\n",
        "#print(confusion.eval(session=sess))\n",
        "\n",
        "pre_test = sess.run(pred,feed_dict={tf_x: X_test.reshape([-1,window_size,1]), tf_y:Y_1hot_test})\n",
        "ytest_lables_argmax = tf.argmax(Y_1hot_test,axis=1)  \n",
        "ytest_pred_argmax = tf.argmax(pre_test,axis=1)\n",
        "confusion = tf.confusion_matrix(labels=ytest_lables_argmax, predictions=ytest_pred_argmax, num_classes=2)\n",
        "print(confusion.eval(session=sess,feed_dict={tf_x: batch_X, tf_y:batch_y}))\n",
        "sess.close()"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py:1266: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file APIs to check for files with this prefix.\n",
            "INFO:tensorflow:Restoring parameters from my_net/save_net_rnn.ckpt\n",
            "Loop0,###iteration: 0###\n",
            "train loss= 0.0706\n",
            "train loss= 0.0706, Acc=0.98\n",
            "Test Acc=0.96\n",
            "[[9882  378]\n",
            " [   0   22]]\n",
            "[[18766  1185]\n",
            " [    0    49]]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}